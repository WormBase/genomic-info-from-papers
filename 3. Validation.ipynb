{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Extracting genes and variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"data/model_output/extracted_snippets.csv\")\n",
    "text = text.to_numpy() # 'WBPaper ID', 'Method', '* Genes', '* Gene-Variant combo', 'Mutation', 'Sentence'\n",
    "\n",
    "OPENING_CLOSING_REGEXES = [r'((?:^|[\\s\\(\\[\\'\"/,;\\-])', r'(?:^|[\\s\\(\\[\\'\"/,;\\-]))']\n",
    "wb_genes = np.load('data/gsoc/wbtools/all_gene_names.npy')\n",
    "all_genes = Path('data/gsoc/wbtools/genes.txt').read_text().split('\\n')\n",
    "for g in wb_genes: all_genes.append(g)\n",
    "all_genes = [g for g in all_genes if len(g) > 1]\n",
    "all_genes = list(set(all_genes))\n",
    "all_genes = OPENING_CLOSING_REGEXES[0] + '|'.join(all_genes) + OPENING_CLOSING_REGEXES[1]\n",
    "all_genes = [re.compile(r,re.IGNORECASE) for r in [all_genes]]\n",
    "\n",
    "# the allele regex and db idea was stolen from wbtools\n",
    "allele_designations = np.load('data/gsoc/wbtools/wb_allele_designations.npy').astype('U6')\n",
    "alleles_variations = np.load('data/gsoc/wbtools/wb_alleles_variations.npy').astype('U6')\n",
    "DB_VAR_REGEX = r'({designations}|m|p|ts|gf|lf|d|sd|am|cs)([0-9]+)'\n",
    "var_regex_1 = OPENING_CLOSING_REGEXES[0] + DB_VAR_REGEX.format(designations=\"|\".join(allele_designations)) + OPENING_CLOSING_REGEXES[1]\n",
    "all_var = OPENING_CLOSING_REGEXES[0] + '|'.join(alleles_variations) + '|' + var_regex_1 + OPENING_CLOSING_REGEXES[1]\n",
    "all_var = [re.compile(r,re.IGNORECASE) for r in [all_var]]\n",
    "\n",
    "# 'WBPaper ID', 'Method', '* Genes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Sentence'\n",
    "data_rows = []\n",
    "total = len(text)\n",
    "print('Total sentences: {}, processed count: '.format(total), end=' ')\n",
    "for i, sent in enumerate(text[:, -1]):\n",
    "    if (i+1) % 50 == 0: print(f\"{i+1}\", end = \" \")\n",
    "    genes = []\n",
    "    for regex in all_genes:      \n",
    "        for m in regex.finditer(sent):\n",
    "            span = (m.start(0), m.end(0))    \n",
    "            raw = (sent[span[0]:span[1]]).strip()\n",
    "            raw = raw[1:] if not raw[0].isalnum() else raw\n",
    "            raw = raw[:-1] if not raw[-1].isalnum() else raw\n",
    "            if len(raw.strip()) > 1: genes.append(raw.strip())\n",
    "    variants = []\n",
    "    for regex in all_var:      \n",
    "        for m in regex.finditer(sent):\n",
    "            span = (m.start(0), m.end(0))    \n",
    "            raw = (sent[span[0]:span[1]]).strip()\n",
    "            raw = raw[1:] if not raw[0].isalnum() else raw\n",
    "            raw = raw[:-1] if not raw[-1].isalnum() else raw\n",
    "            if len(raw.strip()) > 1: variants.append(raw.strip())\n",
    "    if genes:\n",
    "        genes  = list(set(genes))\n",
    "        genes = \"'\" + \"', '\".join(genes) + \"'\"\n",
    "    else:\n",
    "        genes = ''\n",
    "    if variants:\n",
    "        variants  = list(set(variants))\n",
    "        variants = \"'\" + \"', '\".join(variants) + \"'\"\n",
    "    else:\n",
    "        variants = ''\n",
    "    data_rows.append([text[i,0], text[i,1], genes, variants, text[i,2], text[i,3], text[i,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above cell takes a while to complete, so saving the data temporarily\n",
    "data = pd.DataFrame(data_rows[:], columns=['WBPaper ID', 'Method', 'Genes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Sentence'])\n",
    "data.to_csv(\"data/model_output/extracted_snippets_1.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Normalizing genes to WB dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/model_output/extracted_snippets_1.csv\")\n",
    "data = data.to_numpy() # 'WBPaper ID', 'Method', 'Genes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_genes_1 = Path('data/gsoc/Gene_alias.1.txt').read_text().split('\\n')\n",
    "wb_genes_2 = Path('data/gsoc/Gene_alias.2.txt').read_text().split('\\n')\n",
    "wb_genes_3 = Path('data/gsoc/Gene_alias.3.txt').read_text().split('\\n')\n",
    "\n",
    "wb_genes_1 = [r.split('\\t') for r in wb_genes_1]\n",
    "wb_genes_2 = [r.split(' ') for r in wb_genes_2]\n",
    "wb_genes_3 = [r.split(' ') for r in wb_genes_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inefficient way to do this. Have to work on better search algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306123"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wb_genes = dict()\n",
    "\n",
    "for row in wb_genes_1+wb_genes_2+wb_genes_3:\n",
    "    if row[0] not in all_wb_genes.keys():\n",
    "        all_wb_genes[row[0]] = []\n",
    "    for gene in row[1:]: \n",
    "        if len(gene) and gene.lower() not in all_wb_genes[row[0]]: \n",
    "            all_wb_genes[row[0]].append(gene.lower())\n",
    "len(all_wb_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 60, processed count:  10 20 30 40 50 60 "
     ]
    }
   ],
   "source": [
    "print('Total sentences: {}, processed count: '.format(len(data)), end=' ')\n",
    "temp = []\n",
    "\n",
    "for i, genes in enumerate(data[:, 2]):\n",
    "    if (i+1) % 10 == 0: print(f\"{i+1}\", end = \" \")\n",
    "    # checking if nan\n",
    "    if type(genes) == float:\n",
    "        col_genes = ''\n",
    "    else:\n",
    "        genes = genes[1:-1].split(\"', '\")\n",
    "        col_genes = []\n",
    "        \n",
    "        for gene in genes:\n",
    "            for key, value in all_wb_genes.items():\n",
    "                if gene.lower() in value:\n",
    "                    col_genes.append(key)\n",
    "                    break\n",
    "        if col_genes:\n",
    "            col_genes = list(set(col_genes))\n",
    "            col_genes = \"'\" + \"', '\".join(col_genes) + \"'\"\n",
    "        else: \n",
    "            col_genes = ''\n",
    "    temp.append([data[i,0], data[i,1], data[i,2], col_genes, data[i,3], data[i,4], data[i,5], data[i,6]])\n",
    "    \n",
    "data = temp # 'WBPaper ID', 'Method', 'Genes', 'WBGenes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Sentence'\n",
    "temp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if any detected gene was NOT in the WB gene dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 60, 8), dtype='<U1900')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "data[len(data[:,2]) != len(data[:,3])] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Normalizing mutations to  one-letter amino acid codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These code imports would be doing the same thing done in notebook 2.   \n",
    "TODO later, not code breaking: This additional metadata of how the mutation was extracted should be inside notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "from utils.misc.regex_block import mutation_finder_from_regex_filepath, TmVar, CustomWBregex, normalize_mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Rishab/Documents/GitHub/genomic-info-from-papers/genomic-info-from-papers/utils/misc/regex_block.py:302: FutureWarning: Possible nested set at position 15\n",
      "  self._regular_expressions.append(re.compile(reg))\n"
     ]
    }
   ],
   "source": [
    "db_config = configparser.ConfigParser()\n",
    "db_config.read('utils/all_config.cfg')\n",
    "\n",
    "custom_mut_extract = CustomWBregex(db_config, locus_only=True)\n",
    "mf_mut_extract = mutation_finder_from_regex_filepath('data/regexs/mutationfinder_regex/seth_modified.txt')\n",
    "tmvar_mut_extract = TmVar('data/regexs/tmvar_regex/final_regex_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_mut_block(sentence, span_size=150):\n",
    "    mut_and_snippets = []\n",
    "    \n",
    "    # MutationFinder\n",
    "    for mutation, snip in mf_mut_extract(raw_text=sentence, span_size=span_size).items():\n",
    "        mut_and_snippets.append([mutation.OriginalMention, snip])\n",
    "    # tmVar\n",
    "    mut_and_snippets = mut_and_snippets + tmvar_mut_extract(sentence, span_size=span_size)\n",
    "    # Custom patterns\n",
    "    mut_and_snippets = mut_and_snippets + custom_mut_extract(sentence, span_size=span_size)\n",
    "    \n",
    "    if mut_and_snippets:\n",
    "        mut_and_snippets = mut_and_snippets[:][0]\n",
    "        mut_and_snippets = list(set(mut_and_snippets))\n",
    "    return mut_and_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G1110E'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_mutations('G1110 to E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F230L'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_mutations('Phe230Leu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with the protein mutations from regex block for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old - 'WBPaper ID', 'Method', 'Genes', 'WBGenes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Sentence'\n",
    "# new - 'WBPaper ID', 'Method', 'Genes', 'WBGenes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Normalized Mutations', 'Sentence'\n",
    "temp = []\n",
    "for i, row in enumerate(data):\n",
    "    if row[1] != 'Regex':\n",
    "        temp.append(np.insert(data[i], -1, '').tolist())\n",
    "    else:\n",
    "        norm_mutations = []\n",
    "        mutations = data[i, -2][1:-1].split(\"', '\")\n",
    "        for mut in mutations: \n",
    "            mut = point_mut_block(mut)\n",
    "            if mut:\n",
    "                norm_mut = normalize_mutations(mut[0])\n",
    "                norm_mutations.append(norm_mut)\n",
    "        if norm_mutations:\n",
    "            norm_mutations = list(set(norm_mutations))\n",
    "            norm_mutations = \"'\" + \"', '\".join(norm_mutations) + \"'\"\n",
    "        else: \n",
    "            norm_mutations = ''\n",
    "        temp.append(np.insert(data[i], -1, norm_mutations).tolist())\n",
    "        \n",
    "data = temp\n",
    "temp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving things\n",
    "data = pd.DataFrame(data[:], columns=['WBPaper ID', 'Method', 'Genes', 'WBGenes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Normalized Mutations', 'Sentence'])\n",
    "data.to_csv(\"data/model_output/extracted_snippets_2.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/model_output/extracted_snippets_2.csv\")\n",
    "data = data.to_numpy() # 'WBPaper ID', 'Method', 'Genes', 'WBGenes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Normalized Mutations', 'Sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinfa = Path('data/gsoc/proteinfa/c_elegans.PRJNA13758.WS281.protein.fa').read_text().split('>')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19987"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_gene_and_prot = dict() # {wbgene: [transcript, protein]}\n",
    "\n",
    "for row in proteinfa:\n",
    "    wbgene = re.findall(\"WBGene[0-9]+\", row)[0]\n",
    "    protein = \"\".join(re.findall(\"\\n.*\", row)).replace('\\n','')\n",
    "    transcript = row.split(' ')[0]\n",
    "    if wbgene not in wb_gene_and_prot.keys():\n",
    "        wb_gene_and_prot[wbgene] = []\n",
    "    wb_gene_and_prot[wbgene].append([transcript, protein])\n",
    "    \n",
    "len(wb_gene_and_prot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_raw_info_compiled = dict()\n",
    "\n",
    "# 'WBPaper ID', 'Method', 'Genes', 'WBGenes', 'Variants', '*Gene-Variant combo ', 'Mutations', 'Normalized Mutations', 'Sentence'\n",
    "for row in data:\n",
    "    ppr_id = row[0]\n",
    "    norm_muts = row[-2]\n",
    "    wbgenes = row[3]\n",
    "    variants = row[4]\n",
    "    # filtering out nan values\n",
    "    if type(norm_muts) == float:\n",
    "        continue\n",
    "    if ppr_id not in paper_raw_info_compiled.keys():\n",
    "        paper_raw_info_compiled[ppr_id] = {'Mutations':[], 'WBGenes':[], 'Variants':[]}\n",
    "        \n",
    "    norm_muts = norm_muts[1:-1].split(\"', '\")\n",
    "    for m in norm_muts: paper_raw_info_compiled[ppr_id]['Mutations'].append(m)\n",
    "    if type(wbgenes) != float:\n",
    "        wbgenes = wbgenes[1:-1].split(\"', '\")\n",
    "        for w in wbgenes: paper_raw_info_compiled[ppr_id]['WBGenes'].append(w)\n",
    "    if type(variants) != float:\n",
    "        variants = variants[1:-1].split(\"', '\")\n",
    "        for v in variants: paper_raw_info_compiled[ppr_id]['Variants'].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_rows(a):\n",
    "    a = np.ascontiguousarray(a)\n",
    "    unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "    return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [] \n",
    "final_sheet = [] # ppr_id, gene, transcript\n",
    "\n",
    "for ppr_id, info_from_ppr in paper_raw_info_compiled.items():\n",
    "    wbgenes = info_from_ppr['WBGenes']\n",
    "    mutations = info_from_ppr['Mutations']\n",
    "    for gene in wbgenes:\n",
    "        for row in wb_gene_and_prot[gene]:\n",
    "            transcript, protein_string = row\n",
    "            for mut in mutations:\n",
    "                wt_res = mut[0]\n",
    "                pos = int(''.join(n for n in mut if n.isdigit()))\n",
    "                mut_res = mut[-1]\n",
    "                if protein_string[pos-1] == wt_res:\n",
    "                    matches.append([ppr_id, gene + ' ' + mut + ' ' + transcript])\n",
    "matches = unique_rows(matches)\n",
    "for r in matches:\n",
    "    p = r[0]\n",
    "    g, m, t = r[1].split()\n",
    "    final_sheet.append([p,g,m,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WBPaper00006391', 'WBGene00000901', 'E162K', 'W01G7.1'],\n",
       " ['WBPaper00040140', 'WBGene00000123', 'A363V', 'F36A4.7']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving things\n",
    "data = pd.DataFrame(final_sheet[:], columns=['WBPaper ID', 'WBGenes', 'Mutations', 'Transcript'])\n",
    "data.to_csv(\"data/model_output/final_matches.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90bac3f7a4bb879b9d06605bdeda624e0779c88b1a5b8631d7aaa6d430fa2aec"
  },
  "kernelspec": {
   "display_name": "wb_env",
   "language": "python",
   "name": "wb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
