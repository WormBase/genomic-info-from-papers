{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For training BioBERT NER on nala\r\n",
    "Run this in CLI -  \r\n",
    "\r\n",
    "python train_ner.py --model_name_or_path dmis-lab/biobert-base-cased-v1.1 --train_file data/nala/train_dev.json --validation_file data/nala/devel.json --text_column_name tokens --label_column_name tags --pad_to_max_length --max_length 192 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 10 --output_dir models/nala --seed 1\r\n",
    "  \r\n",
    "Note:  Training took <30 mins on 1660 ti. You can decrease the num_train_epochs count to 5 without any substantial difference in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Regex \r\n",
    "## Has high precision and low recall.    \r\n",
    "## Consists of 3 parts - MutationFinder, tmVar and some custom patterns from WB papers \r\n",
    "### 1.1 Mutation Finder [Link](https://github.com/divyashan/MutationFinder), Modified regex from SETH [Link](https://github.com/rockt/SETH/blob/master/resources/mutations.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc.regex_block import mutation_finder_from_regex_filepath, TmVar, BOWdictionary, CustomWBregex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_regex_path = 'data/regexs/mutationfinder_regex/seth_modified.txt'\r\n",
    "mf_mut_extract = mutation_finder_from_regex_filepath(mf_regex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(1154)C : A(1154)C\n"
     ]
    }
   ],
   "source": [
    "text = 'A(1154)C'\r\n",
    "for mutation, snip in mf_mut_extract(raw_text=text, span_size=150).items():\r\n",
    "    mutation_entry = snip + ' : ' + mutation.OriginalMention\r\n",
    "    print(mutation_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 tmVar [Link](https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/tmvar/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Rishab/Documents/GitHub/genomic-info-from-papers/genomic-info-from-papers/utils/misc/regex_block.py:302: FutureWarning: Possible nested set at position 15\n",
      "  self._regular_expressions.append(re.compile(reg))\n"
     ]
    }
   ],
   "source": [
    "tmvar_mut_extract = TmVar('data/regexs/tmvar_regex/final_regex_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A347V', ' n2923 (A347V), n2870(R429K), and n1163(S486F'],\n",
       " ['R429K', ' n2923 (A347V), n2870(R429K), and n1163(S486F']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmvar_mut_extract(' n2923 (A347V), n2870(R429K), and n1163(S486F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extra custom regexs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utils/all_config.cfg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_config = configparser.ConfigParser()\r\n",
    "db_config.read('utils/all_config.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mut_extract = CustomWBregex(db_config, extra_regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the statement ced-3(n2888) \r\n",
    "n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT',\n",
       "  'n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_mut_extract('n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ced-3', 'Just gene']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_mut_extract.get_genes('ced-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_mut_extract = BOWdictionary()\r\n",
    "# bow_mut_extract('This mutation deletes 471bp of the promoter region, the transcriptional start and 56 amino acids of the second exon.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 MF + tmVar + Custom regex + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_rows(a):\n",
    "    a = np.ascontiguousarray(a)\n",
    "    unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "    return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))\n",
    "\n",
    "\n",
    "def regex_block(sentence, span_size=150):\n",
    "    mut_and_snippets = []\n",
    "    \n",
    "    # MutationFinder\n",
    "    for mutation, snip in mf_mut_extract(raw_text=sentence, span_size=span_size).items():\n",
    "        mut_and_snippets.append([mutation.OriginalMention, snip])\n",
    "    \n",
    "    # tmVar\n",
    "    mut_and_snippets = mut_and_snippets + tmvar_mut_extract(sentence, span_size=span_size)\n",
    "    # Custom patterns\n",
    "    mut_and_snippets = mut_and_snippets + custom_mut_extract(sentence, span_size=span_size)\n",
    "    # Bag of words\n",
    "    mut_and_snippets = mut_and_snippets + bow_mut_extract(sentence)\n",
    "    \n",
    "    if mut_and_snippets:\n",
    "        mut_and_snippets = unique_rows(mut_and_snippets).tolist()\n",
    "    return mut_and_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_block(' asdf gpa-2 ::Tc1 asdf as')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 * Additional details  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_info_block(sentence, span_size=150):\r\n",
    "    info_and_snippets = []\r\n",
    "\r\n",
    "    # look for gene and variant combo\r\n",
    "    info_and_snippets = info_and_snippets + custom_mut_extract.var_and_gene_close(sentence, span_size=span_size)\r\n",
    "    \r\n",
    "    if info_and_snippets:\r\n",
    "        info_and_snippets = unique_rows(info_and_snippets).tolist()\r\n",
    "    return info_and_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ced-3(n2888', 'Gene & Variant']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_info_block('in the statement ced-3(n2888)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in scores\r\n",
    "MF + tmVar only: on remarks - (0.9459459459459459, 0.39923954372623577, 0.5614973262032086, None)   \r\n",
    "on remarks + nala - (0.8671477079796265, 0.6698360655737705, 0.755826859045505, None)  \r\n",
    "    \r\n",
    "MF + tmVar + Custom + BOW: on remarks - (0.9606741573033708, 0.6501901140684411, 0.7755102040816326, None)  \r\n",
    "on remarks + nala - (0.8680868496517821, 0.6947540983606557, 0.7718084137679839, None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 BioBERT NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/risubu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\r\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\r\n",
    "from transformers import TokenClassificationPipeline\r\n",
    "\r\n",
    "import re\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import sklearn as sk\r\n",
    "import math \r\n",
    "import string\r\n",
    "import time\r\n",
    "import json\r\n",
    "import csv\r\n",
    "import shutil\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'models/nala'\r\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n",
    "model = AutoModelForTokenClassification.from_pretrained(\r\n",
    "    model_name_or_path,\r\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\r\n",
    "    config=config,\r\n",
    ")\r\n",
    "# LABEL_0 - B-mut, LABEL_1 - I-mut, LABEL_2 - O\r\n",
    "nala_ner  = TokenClassificationPipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\r\n",
    "stop_words = [w for w in stop_words if len(w) > 1]\r\n",
    "\r\n",
    "def ner_score(sentence):\r\n",
    "    mutations = []\r\n",
    "    try:\r\n",
    "        ner_output = nala_ner(sentence)\r\n",
    "        for i, grp in enumerate(ner_output):\r\n",
    "            if grp['entity_group'] == 'LABEL_0':\r\n",
    "                mut = grp['word']\r\n",
    "                for j in range(i+1, len(ner_output)):\r\n",
    "                    if ner_output[j]['entity_group'] == 'LABEL_1':\r\n",
    "                        mut  = mut + ' ' + ner_output[j]['word']\r\n",
    "                    else:\r\n",
    "                        # NER would be handling only data in NL form\r\n",
    "                        if len(mut.split()) > 3 and any(word in mut.split() for word in stop_words):\r\n",
    "                            mutations.append([mut, sentence])\r\n",
    "                        break\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    return mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Testing \r\n",
    "## 3.2 On scored curator remarks and nala mutation corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_texts = []\r\n",
    "# issues_count = 0\r\n",
    "\r\n",
    "# df = pd.read_csv(r\"data\\gsoc\\Remarks_scored.csv\")\r\n",
    "# df = df.to_numpy()\r\n",
    "# # extract sentences\r\n",
    "# text = []\r\n",
    "# y_true = []\r\n",
    "# for idx, row in enumerate(df):\r\n",
    "#     loc = str(row[2]).find('Paper_evidence')\r\n",
    "#     if loc != -1:\r\n",
    "#         if row[0].split()[0] == 'Yes':\r\n",
    "#             y_true.append(1)\r\n",
    "#         elif row[0].split()[0] == 'No':\r\n",
    "#             y_true.append(0)\r\n",
    "#         else:\r\n",
    "#             continue\r\n",
    "#         temp_str = str(row[2][1:loc-2]).replace(\"\\\"\", \"'\")\r\n",
    "#         text.append(temp_str)\r\n",
    "\r\n",
    "# assert len(text) == len(y_true)\r\n",
    "# print('Count from Remarks_scored = {}'.format(len(text)))\r\n",
    "\r\n",
    "# y_pred = []\r\n",
    "# for sentence in text:\r\n",
    "#     if regex_block(sentence):\r\n",
    "#         y_pred.append(1)\r\n",
    "#     elif ner_score(sentence):\r\n",
    "#         y_pred.append(1)\r\n",
    "#     else:\r\n",
    "#         y_pred.append(0)\r\n",
    "\r\n",
    "# assert len(y_pred) == len(y_true)\r\n",
    "# all_texts = all_texts + text\r\n",
    "\r\n",
    "\r\n",
    "# df = pd.read_csv(r\"data\\nala\\binary_nala_NOT_NER.csv\")\r\n",
    "# df = df.to_numpy()\r\n",
    "# print('Count from nala = {}'.format(len(df)))\r\n",
    "\r\n",
    "# print('Entries processed: ', end=' ')\r\n",
    "# for i, row in enumerate(df):\r\n",
    "#     if i%500 == 0: print(i, end=' ')\r\n",
    "#     sentence = row[0]\r\n",
    "#     label = row[1]\r\n",
    "#     try:\r\n",
    "#         if regex_block(sentence):\r\n",
    "#             y_pred.append(1)\r\n",
    "#         elif ner_score(sentence):\r\n",
    "#             y_pred.append(1)\r\n",
    "#         else:\r\n",
    "#             y_pred.append(0)\r\n",
    "\r\n",
    "#         if label == 1:\r\n",
    "#             y_true.append(1)\r\n",
    "#         else:\r\n",
    "#             y_true.append(0)\r\n",
    "        \r\n",
    "#         all_texts.append(sentence)\r\n",
    "#     except:\r\n",
    "#         issues_count += 1\r\n",
    "#         pass\r\n",
    "\r\n",
    "# assert len(y_pred) == len(y_true) == len(all_texts)\r\n",
    "# print('\\nTotal count = {}'.format(len(y_pred)))\r\n",
    "# if issues_count: print('Note: Could not process {} sentences'.format(issues_count))\r\n",
    "\r\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually inspecting incorrect preds\r\n",
    "# bad = []\r\n",
    "# assert len(y_pred) == len(y_true) == len(all_texts)\r\n",
    "# for i, (t, p,sent) in enumerate(zip(y_true, y_pred, all_texts)):\r\n",
    "#     if t != p:\r\n",
    "#         if t:\r\n",
    "#             bad.append([sent,i])\r\n",
    "# len(bad)\r\n",
    "# # print(bad[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in scores (find the test cell block below)\r\n",
    "  \r\n",
    "Regex only: on remarks - (0.9606741573033708, 0.6501901140684411, 0.7755102040816326, None)  \r\n",
    "on remarks + nala - (0.8680868496517821, 0.6947540983606557, 0.7718084137679839, None) \r\n",
    "  \r\n",
    "Regex + NER: on remarks - (0.8592057761732852, 0.9049429657794676, 0.8814814814814815, None)   \r\n",
    "on remarks + nala - (0.7570694087403599, 0.9655737704918033, 0.8487031700288185, None)  \r\n",
    "  \r\n",
    "### Retraining on NL and SST  \r\n",
    "Regex + NER: on remarks - (0.9627906976744186, 0.7870722433460076, 0.8661087866108786, None)      \r\n",
    "on remarks + nala - (0.8461816865725661, 0.7665573770491804, 0.804403922243248, None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 On WB papers\r\n",
    "### Get the paper texts from textpresso API and wbtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works - First paper ID is searched through textpresso API. If the recieved output is blank, then wbtools is used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import argparse\n",
    "import subprocess\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import nltk.data\n",
    "import os\n",
    "from xml.dom import minidom\n",
    "from bs4 import BeautifulSoup\n",
    "from wbtools.literature.corpus import CorpusManager\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the cell below only once to save the paper sentences in your local computer to avoid pulling paper text everytime during dev  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreate a separate temporary numpy file to store the paper sentences  \\nto avoid spending time on pulling sentences while in development \\nFormat - [paper_id, sentence]\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create a separate temporary numpy file to store the paper sentences  \n",
    "to avoid spending time on pulling sentences while in development \n",
    "Format - [paper_id, sentence]\n",
    "'''\n",
    "# final_data = []\n",
    "# # remove_sections = [PaperSections.ACKNOWLEDGEMENTS, PaperSections.REFERENCES, PaperSections.RELATED_WORK, PaperSections.INTRODUCTION]\n",
    "# remove_sections = []\n",
    "# # random 100 papers mentioned the remarks ace file in data/gsoc\n",
    "# paper_ids = np.load('data/top100.npy').tolist()\n",
    "# cm = CorpusManager()\n",
    "# for i, paper_id in enumerate(paper_ids):\n",
    "#     paper_id = paper_id[7:]\n",
    "#     cm.load_from_wb_database(db_name=db_config['wb_database']['db_name'], db_user=db_config['wb_database']['db_user'], db_password=db_config['wb_database']['db_password'],\n",
    "#         db_host=db_config['wb_database']['db_host'], paper_ids=[paper_id],\n",
    "#         ssh_host=db_config['wb_database']['ssh_host'], ssh_user=db_config['wb_database']['ssh_user'], ssh_passwd=db_config['wb_database']['ssh_passwd'],\n",
    "#         load_bib_info=False, load_afp_info=False, load_curation_info=False)\n",
    "#     sentences = cm.get_paper(paper_id).get_text_docs(remove_sections=remove_sections,split_sentences=True)\n",
    "#     for sent in sentences:\n",
    "#         final_data.append([paper_id, sent])\n",
    "#     print(i, end = \" \")\n",
    "# final_data = pd.DataFrame(final_data[:], columns=['WBPaper ID', 'Sentence'])\n",
    "# final_data.to_csv(\"data/id_and_sentence.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textpresso_paper_text(wbpid, path, token):\n",
    "    \"\"\"This sub takes a wbpid eg WBPaper00056731 and returns the fulltext paper in sentences\"\"\"\n",
    "    ft=[0];\n",
    "    # Check that wbpid is a valid WBPaper\n",
    "    if not re.match( 'WBPaper', wbpid):\n",
    "        print (wbpid, \"is not a valid WBPaper ID\")\n",
    "        return ft\n",
    "    # Download paper if it doesn't exist\n",
    "    fn = path + '/temp/' + wbpid + '.json'\n",
    "\n",
    "    if os.path.exists(fn) and os.path.getsize(fn) > 16:\n",
    "        pass\n",
    "    else:\n",
    "        com1 = '-o '+fn +'\\n-k '+ '\\n'+'-d \"{\\\\\"token\\\\\":\\\\\"'+ token + '\\\\\", \\\\\"query\\\\\": {\\\\\"accession\\\\\": \\\\\"' + wbpid +'\\\\\", \\\\\"type\\\\\": \\\\\"document\\\\\", \\\\\"corpora\\\\\": [\\\\\"C. elegans\\\\\"]}, \\\\\"include_fulltext\\\\\": true}\"'\n",
    "        configf= path +'/temp/' + wbpid + '.tmp.config'\n",
    "        curlf = open(configf,'w')\n",
    "        print (com1, file=curlf)\n",
    "        curlf.close()\n",
    "        command = 'curl -o '+ fn +' -K '+ configf+' https://textpressocentral.org:18080/v1/textpresso/api/search_documents' \n",
    "        comlist = command.split()\n",
    "        os.system(command)\n",
    "\n",
    "    # Read the paper, and split into sentences\n",
    "    if os.path.exists(fn) and os.path.getsize(fn) > 20:\n",
    "        # Open our JSON file and load it into python\n",
    "        input_file = open (fn)\n",
    "        json_array = json.load(input_file)\n",
    "        for item in json_array:\n",
    "            abs = item[\"abstract\"]\n",
    "            fullt =  item[\"fulltext\"]\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            ft = tokenizer.tokenize(abs)\n",
    "            ftt=tokenizer.tokenize(fullt)\n",
    "            ft = ft +ftt\n",
    "    else:\n",
    "        # some paper texts are blank for some reason\n",
    "        # pipeline uses wbtools to get sentences in such case\n",
    "        pass\n",
    "\n",
    "    outfilen = os.path.join(path, 'text_flatfiles', wbpid+'.txt')\n",
    "    outf = open(outfilen, 'w')\n",
    "    for sen in ft:\n",
    "        sen =str(sen)\n",
    "        print(sen, file=outf)\n",
    "    outf.close()\n",
    "    return outfilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentences out of wbtools are sometimes weird, especially for the old papers\n",
    "This increases the false neg rate so this is used only when the textpresso api provides bad text\n",
    "Most of the issues have band aid fixes in the text preprocessing cell below but there are some \n",
    "with no easy fix or need to be worked on - \n",
    "1. Table content is extracted column wise, not row wise \n",
    "2. Some sentences have white space between every character and are also somehow inverted (???)\n",
    "    e.g. Check out the sentences of WBPaper00002018 (1994)\n",
    "    Line 154 -  '0 7 1 u ( 7 - c e m    ) F (   .'\n",
    "    inverted and without the white space is (mec-7)u170 which is extremely useful and will get missed \n",
    "    by the pipeline unless processed correctly.\n",
    "TODO: Not sure how to solve point 1 but point 2 is easy to solve and also helps a LOT.\n",
    "    Rishab, work on this after you complete the project. \n",
    "    Not high priority as this might be only for the >10 year old papers (which are already manually curated)\n",
    "'''\n",
    "cm = CorpusManager()\n",
    "def wbtools_paper_text(wbpid, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "    ssh_user, ssh_passwd):\n",
    "    # sectioning might not be always correct, text processing is done separately in the pipeline\n",
    "    # remove_sections = [PaperSections.ACKNOWLEDGEMENTS, PaperSections.REFERENCES, PaperSections.RELATED_WORK, PaperSections.INTRODUCTION]\n",
    "    remove_sections = []\n",
    "    paper_id = wbpid[7:]\n",
    "    cm.load_from_wb_database(db_name=db_name, db_user=db_user, db_password=db_password,\n",
    "        db_host=db_host, paper_ids=[paper_id],\n",
    "        ssh_host=ssh_host, ssh_user=ssh_user, ssh_passwd=ssh_passwd,\n",
    "        load_bib_info=False, load_afp_info=False, load_curation_info=False)\n",
    "    sentences = cm.get_paper(paper_id).get_text_docs(remove_sections=remove_sections,split_sentences=True)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_sentences(wbpids, config, store_ppr_path):\n",
    "    '''\n",
    "    Takes WB Paper IDs and returns a list of sentences from those papers after filtering\n",
    "    Arg:\n",
    "    wbpids - List of wb papers ids \n",
    "        e.g. ['WBPaper00002379']\n",
    "    config_path - Config file path\n",
    "    store_ppr_path - Folder path to store the paper flatfiles retrieved from TextPresso for future use\n",
    "    Returns:\n",
    "    paperid_sentence_list: List of paper ID and sentence\n",
    "        e.g. [['WBPaper00002379', 'First sentence'], ['WBPaper00002379', 'Second sentence'], ....]\n",
    "    '''\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words = [w for w in stop_words if len(w) > 1]\n",
    "\n",
    "    all_special_chars = []\n",
    "    with open('data/nala/train_dev.json') as f:\n",
    "        for jsonObj in f:\n",
    "            nala_json = json.loads(jsonObj)['tokens']\n",
    "            for word in nala_json:\n",
    "                if not word.isalnum():\n",
    "                    all_special_chars.append(word)\n",
    "    # list of special characters to keep during inference\n",
    "    # helps with clearing out the bad characters from old papers\n",
    "    all_special_chars = list(set(all_special_chars))\n",
    "    \n",
    "    token = config['textpresso']['token']\n",
    "    db_name=config['wb_database']['db_name']\n",
    "    db_user=config['wb_database']['db_user']\n",
    "    db_password=config['wb_database']['db_password']\n",
    "    db_host=config['wb_database']['db_host']\n",
    "    ssh_host=config['wb_database']['ssh_host']\n",
    "    ssh_user=config['wb_database']['ssh_user']\n",
    "    ssh_passwd=config['wb_database']['ssh_passwd']\n",
    "\n",
    "    temp_paperid_sentence = np.array([])\n",
    "    if os.path.isfile('data/id_and_sentence.csv'):\n",
    "        temp_paperid_sentence = pd.read_csv(\"data/id_and_sentence.csv\", lineterminator='\\n', dtype = str).to_numpy() # WBPaper ID, Sentence\n",
    "    paperid_sentence_list = np.array([['WBPaperID', 'Sentence']])\n",
    "\n",
    "    for curr_ppr_i, id in enumerate(wbpids):\n",
    "        print(f\"{curr_ppr_i+1}\", end = \" \")\n",
    "        # textpresso_paper_text() also saves the text in flatfiles for future use \n",
    "        paper_path = textpresso_paper_text(id, store_ppr_path, token)\n",
    "        txt = Path(paper_path).read_text().split('\\n')\n",
    "        # deals with empty text files with only \"0\"\n",
    "        if len(txt) == 2:\n",
    "            if temp_paperid_sentence.size != 0:\n",
    "                txt = temp_paperid_sentence[temp_paperid_sentence[:, 0] == id[7:]][:, 1]\n",
    "                # incase the loaded numpy file didn't have the required paper \n",
    "                if len(txt) == 0 and platform.system() != 'Windows':\n",
    "                    print(123)\n",
    "                    txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                        ssh_user, ssh_passwd)\n",
    "            elif platform.system() != 'Windows' :\n",
    "                txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                    ssh_user, ssh_passwd)\n",
    "            \n",
    "        for row in txt: \n",
    "            if row.find('fifi') != -1:\n",
    "                if temp_paperid_sentence.size != 0:\n",
    "                    txt = temp_paperid_sentence[temp_paperid_sentence[:, 0] == id[7:]][:, 1]\n",
    "                    # incase the loaded numpy file didn't have the required paper \n",
    "                    if len(txt) == 0 and platform.system() != 'Windows':\n",
    "                        print(456)\n",
    "                        txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                            ssh_user, ssh_passwd)\n",
    "                elif platform.system() != 'Windows':\n",
    "                    txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                        ssh_user, ssh_passwd)\n",
    "                break\n",
    "            \n",
    "        count_total_rows = len(txt)\n",
    "        for current_i, row in enumerate(txt):\n",
    "            if row.lower().find(\"we thank\") == 0 or row.lower().find(\"this work was supported\") == 0 \\\n",
    "                or row.lower().find(\"references\") == 0 or row.lower().find(\"we also thank\") == 0 \\\n",
    "                or row.lower().find(\"this research was supported\") == 0 or row.lower().find(\"we acknowledge\") == 0 \\\n",
    "                or row.lower().find(\"acknowledgments\") == 0 or row.lower().find('literature cited') != -1:\n",
    "                if current_i > count_total_rows/2:\n",
    "                    break\n",
    "\n",
    "            # usually is bad sentence\n",
    "            if len(row) < 40 or not any(word in row.lower().split() for word in stop_words):\n",
    "                continue\n",
    "            # remove sentences with links and email ids\n",
    "            if re.search('\\S+@\\S+\\.', row) or re.search('www.\\S+\\.', row):\n",
    "                continue\n",
    "            # filters one word sentences\n",
    "            if len(row.split()) == 1:\n",
    "                continue\n",
    "            # sentences comprised of only single characters \n",
    "            # ^ seems to be issue with wbtools extraction pipeline \n",
    "            if all(len(word) < 5 for word in row.split()):\n",
    "                continue\n",
    "            row = re.sub(\"\\( *cid *: *\\d+ *\\)\", \" \", row)\n",
    "            temp_row = row\n",
    "            for c in temp_row:\n",
    "                if (not c.isalnum() and not c == ' ') and c not in all_special_chars:\n",
    "                        row = row.replace(c, \"\")\n",
    "            # fixes bad space between each character of flanking sequence from old papers\n",
    "            flanking_regex = re.compile('([ACTG]( +)){4,}', re.IGNORECASE)\n",
    "            for m in flanking_regex.finditer(row):\n",
    "                span = (m.start(0), m.end(0))   \n",
    "                span = row[span[0]:span[1]-1]\n",
    "                correct_flank = re.sub('([ACTG])( +)', r'\\1', row, flags=re.I)\n",
    "                row = row.replace(span, correct_flank)\n",
    "\n",
    "            # filters out repeated lines, e.g. check out WBPaper00028727.txt in flatfiles folder\n",
    "            if row not in paperid_sentence_list[paperid_sentence_list[:,0]==id][:,1]:\n",
    "                paperid_sentence_list = np.vstack((paperid_sentence_list, [id, row]))\n",
    "    return paperid_sentence_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 "
     ]
    }
   ],
   "source": [
    "# papers mentioned the remarks ace file in data/gsoc\n",
    "ids_to_extract = np.load('data/top100.npy').tolist()[-80:-10]\n",
    "paperid_sentence_list = get_paper_sentences(ids_to_extract, db_config, store_ppr_path='data/wbpapers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sentences and characters: ', end=' ')\n",
    "print(len(paperid_sentence_list), sum([len(sent[1]) for sent in paperid_sentence_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell takes a while to run - mainly due to the huge regex blocks   \n",
    "~ 2 seconds per sentence , 1 hour per 10 papers   \n",
    "Can't really switch them off though. There might be a smarter way to work with the regex block but this pipeline would be probably running every month on 50 or so papers (around 8 hours) so low priority.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "final = [\n",
    "    ['temporary', 'temporary', 'temporary', 'temporary', 'temporary', 'temporary'],\\\n",
    "    ['WBPaper ID', 'Method', '*Genes', '*Gene-Variant combo', 'Mutation', 'Sentence']]\n",
    "total_sentences = len(paperid_sentence_list)\n",
    "print_snips = False\n",
    "print('Total sentences to process ', len(paperid_sentence_list))\n",
    "print('{total sentences processed}>{snippets with useful info}')\n",
    "for ppr_sen_idx, row in enumerate(paperid_sentence_list):\n",
    "    if (ppr_sen_idx+1) % 50 == 0: print(f\"{ppr_sen_idx+1}>{len(final)-1}\", end = \" \")\n",
    "    paper_id = row[0]\n",
    "    sentence = str()\n",
    "    limit = min(ppr_sen_idx+2, total_sentences)\n",
    "    # some sentences - mostly table content are super long\n",
    "    # temp fix, need to have a nice sentence splitter to minimize manual verification time\n",
    "    not_single_sentence = False\n",
    "    for i in range(ppr_sen_idx, limit):\n",
    "\n",
    "        sentence = sentence + paperid_sentence_list[i][1] + ' '\n",
    "\n",
    "        if (len(sentence) > 250 and not_single_sentence):\n",
    "            break\n",
    "        if paper_id != paperid_sentence_list[i][0]:\n",
    "            break\n",
    "        \n",
    "        var_plus_genes = ''\n",
    "        # Look for the special data e.g. gene-variant combo (e.g 'ced-3(n2888)') only on single sentences \n",
    "        if not not_single_sentence:\n",
    "            var_plus_genes = []\n",
    "            all_genes = []\n",
    "            \n",
    "            for data_and_cat in custom_mut_extract.var_and_gene_close(sentence.strip()):\n",
    "                var_plus_genes.append(data_and_cat[0])\n",
    "            if var_plus_genes:\n",
    "                var_plus_genes  = list(set(var_plus_genes))\n",
    "                var_plus_genes = \"'\" + \"', '\".join(var_plus_genes) + \"'\"\n",
    "            else:\n",
    "                var_plus_genes = ''\n",
    "                \n",
    "            for data_and_cat in custom_mut_extract.get_genes(sentence.strip()):\n",
    "                all_genes.append(data_and_cat[0])\n",
    "            if all_genes:\n",
    "                all_genes  = list(set(all_genes))\n",
    "                all_genes = \"'\" + \"', '\".join(all_genes) + \"'\"\n",
    "            else:\n",
    "                all_genes = ''   \n",
    "                \n",
    "        output = regex_block(sentence.strip())\n",
    "        if output:\n",
    "            mutations = []\n",
    "            for mut_and_snip in output:\n",
    "                # temp fix to deal with same mutation getting detected due to stiching multiple sentences\n",
    "                if (mut_and_snip[0] not in final[-1][4][1:-1].split(\", \") and mut_and_snip[0] not in final[-2][4][1:-1].split(\", \")) \\\n",
    "                            and mut_and_snip[0] not in mutations:\n",
    "                    mutations.append(mut_and_snip[0])\n",
    "            if mutations:\n",
    "                mutations = \"'\" + \"', '\".join(mutations) + \"'\"\n",
    "                if print_snips: print(1, mutations)\n",
    "                final.append([paper_id, 'Regex', all_genes, var_plus_genes, mutations, 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "            break\n",
    "\n",
    "        output = ner_score(sentence.strip())\n",
    "        if output:\n",
    "            mutations = []\n",
    "            for mut_and_snip in output:\n",
    "                # temp fix to deal with same mutation getting detected due to stiching multiple sentences\n",
    "                if (mut_and_snip[0] not in final[-1][4][1:-1].split(\", \") and mut_and_snip[0] not in final[-2][4][1:-1].split(\", \")) \\\n",
    "                        and not all(len(word) < 4 for word in mut_and_snip[0].split())\\\n",
    "                    and mut_and_snip[0] not in mutations:\n",
    "                    mutations.append(mut_and_snip[0])\n",
    "            if mutations:\n",
    "                mutations = \"'\" + \"', '\".join(mutations) + \"'\"\n",
    "                if print_snips: print(2, mutations)\n",
    "                final.append([paper_id, 'NER', all_genes, var_plus_genes, mutations, 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "            break\n",
    "        \n",
    "        # these data, if found, are going to be important if no mutations are in that sentence\n",
    "        if var_plus_genes or all_genes:\n",
    "            final.append([paper_id, '', all_genes, var_plus_genes, '', 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "        \n",
    "        not_single_sentence = True\n",
    "print('Total time for processing in minutes: ', int((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final[2:] # removing the temporary first row and header\n",
    "\n",
    "# columns with asterisk contain data which is useful regardless of whether the sentence has  mutation info\n",
    "data = pd.DataFrame(temp[:], columns=['WBPaper ID', 'Method', '* Genes', '* Gene-Variant combo', 'Mutation', 'Sentence'])\n",
    "data.to_csv(\"data/model_output/extracted_snippets_part2.csv\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90bac3f7a4bb879b9d06605bdeda624e0779c88b1a5b8631d7aaa6d430fa2aec"
  },
  "kernelspec": {
   "display_name": "wb_env",
   "language": "python",
   "name": "wb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
