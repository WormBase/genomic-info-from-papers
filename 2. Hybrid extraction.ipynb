{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# For training BioBERT NER on nala\r\n",
    "Run this in CLI -  \r\n",
    "\r\n",
    "python train_ner.py --model_name_or_path dmis-lab/biobert-base-cased-v1.1 --train_file data/nala/train_dev.json --validation_file data/nala/devel.json --text_column_name tokens --label_column_name tags --pad_to_max_length --max_length 192 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 10 --output_dir models/nala --seed 1\r\n",
    "  \r\n",
    "Note:  Training took <30 mins on 1660 ti. You can decrease the num_train_epochs count to 5 without any substantial difference in accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Regex \r\n",
    "## Has high precision and low recall.    \r\n",
    "## Consists of 3 parts - MutationFinder, tmVar and some custom patterns from WB papers \r\n",
    "### 1.1 Mutation Finder [Link](https://github.com/divyashan/MutationFinder), Modified regex from SETH [Link](https://github.com/rockt/SETH/blob/master/resources/mutations.txt)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\r\n",
    "import os\r\n",
    " \r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# A dictionary mapping three-letter amino acids codes onto one-letter\r\n",
    "# amino acid codes\r\n",
    "amino_acid_three_to_one_letter_map = \\\r\n",
    "    dict([('ALA','A'),('GLY','G'),('LEU','L'),('MET','M'),\\\r\n",
    "     ('PHE','F'),('TRP','W'),('LYS','K'),('GLN','Q'),('GLU','E'),('SER','S'),\\\r\n",
    "     ('PRO','P'),('VAL','V'),('ILE','I'),('CYS','C'),('TYR','Y'),('HIS','H'),\\\r\n",
    "     ('ARG','R'),('ASN','N'),('ASP','D'),('THR','T'),('XAA','X'),('GLX','Z'),\\\r\n",
    "     ('ASX','B'), ('TER', 'X'), ('STP', 'X')])\r\n",
    "\r\n",
    "# A dictionary mapping amino acid names to their one-letter abbreviations\r\n",
    "amino_acid_name_to_one_letter_map = \\\r\n",
    "    dict([('ALANINE','A'),('GLYCINE','G'),('LEUCINE','L'),\\\r\n",
    "     ('METHIONINE','M'),('PHENYLALANINE','F'),('TRYPTOPHAN','W'),\\\r\n",
    "     ('LYSINE','K'),('GLUTAMINE','Q'),('GLUTAMIC ACID','E'),\\\r\n",
    "     ('GLUTAMATE','E'),('ASPARTATE','D'),('SERINE','S'),\\\r\n",
    "     ('PROLINE','P'),('VALINE','V'),('ISOLEUCINE','I'),('CYSTEINE','C'),\\\r\n",
    "     ('TYROSINE','Y'),('HISTIDINE','H'),('ARGININE','R'),\\\r\n",
    "     ('ASPARAGINE','N'),('ASPARTIC ACID','D'),('THREONINE','T'), \\\r\n",
    "     ('OCHRE', 'X'), ('AMBER', 'X'), ('OPAL', 'X'), ('UMBER', 'X'), \\\r\n",
    "     ('STOP', 'X'), ('TERM', 'X'), ('*', '*')])\r\n",
    "     \r\n",
    "class MutationError(Exception):\r\n",
    "        pass\r\n",
    "\r\n",
    "class Mutation(object):\r\n",
    "    \"\"\" A base class for storing information about mutations \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, Position):\r\n",
    "        \"\"\" Initalize the object\r\n",
    "\r\n",
    "            Position: the sequence position or start position of the mutation\r\n",
    "                (must be castable to an int)\r\n",
    "        \"\"\"\r\n",
    "        try:\r\n",
    "            self.__position = int(Position)\r\n",
    "        except ValueError:\r\n",
    "            self.__position = '<See original snippet for number>'\r\n",
    "            # NOTE: commented below lines due to inconsistency in the MF created regex\r\n",
    "            # raise MutationError(\"Position must be an integer\")\r\n",
    "        # if self.__position < 1:\r\n",
    "        #     raise MutationError(\"Position must be greater than 0\")\r\n",
    "\r\n",
    "    def _get_position(self):\r\n",
    "        return self.__position\r\n",
    "    Position = property(_get_position)\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        raise NotImplementedError('Mutation subclasses must override str()')\r\n",
    "\r\n",
    "    def __eq__(self, other):\r\n",
    "        raise NotImplementedError('Mutation subclasses must override ==')\r\n",
    "\r\n",
    "    def __ne__(self, other):\r\n",
    "        raise NotImplementedError('Mutation subclasses must override !-')\r\n",
    "\r\n",
    "    def __hash__(self):\r\n",
    "        raise NotImplementedError('Mutation subclasses must override hash()')\r\n",
    "\r\n",
    "# TODO: very cluttery. Refactor later pls\r\n",
    "# _normalize_residue_identity() fn can be used directly\r\n",
    "class PointMutation(Mutation):\r\n",
    "    \"\"\" \r\n",
    "    A class for storing information about protein point mutations\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Define a mapping for residue identity inputs to one-letter\r\n",
    "    # abbreviations. For simplicty of the normalization procedure, a\r\n",
    "    # one-letter to one-letter 'mapping' is also included. This\r\n",
    "    # eliminates the need for an independent validation step, since\r\n",
    "    # any valid identity which is passed in will be a key in this dict,\r\n",
    "    # and it avoids having to analyze which format the input residue\r\n",
    "    # was passed in as.\r\n",
    "    _abbreviation_lookup = dict(zip(list('ABCDEFGHIKLMNPQRSTVWXYZ'),\r\n",
    "                                    list('ABCDEFGHIKLMNPQRSTVWXYZ')))\r\n",
    "    _abbreviation_lookup.update(amino_acid_three_to_one_letter_map)\r\n",
    "    _abbreviation_lookup.update(amino_acid_name_to_one_letter_map)\r\n",
    "\r\n",
    "    def __init__(self, Position, WtResidue, MutResidue, originalMention):\r\n",
    "        \"\"\" Initalize the object and call the base class init\r\n",
    "\r\n",
    "            Position: the sequence position or start position of the mutation\r\n",
    "                (castable to an int)\r\n",
    "            WtResidue: the wild-type (pre-mutation) residue identity (a string)\r\n",
    "            MutReside: the mutant (post-mutation) residue identity (a string)\r\n",
    "\r\n",
    "            Residues identities are validated to ensure that they are within\r\n",
    "             the canonical set of amino acid residues are normalized to their\r\n",
    "             one-letter abbreviations.\r\n",
    "        \"\"\"\r\n",
    "        self.__wt_residue = self._normalize_residue_identity(WtResidue)\r\n",
    "        self.__mut_residue = self._normalize_residue_identity(MutResidue)\r\n",
    "        self.__original_mention = originalMention\r\n",
    "        Mutation.__init__(self, Position=Position)\r\n",
    "\r\n",
    "    def _normalize_residue_identity(self, residue):\r\n",
    "        \"\"\" Normalize three-letter and full residue names to their\r\n",
    "             one-letter abbreviations. If a residue identity is passed in\r\n",
    "             which does not fall into the set of canonical amino acids\r\n",
    "             a MutationError is raised.\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        try:\r\n",
    "                # convert residue to its single letter abbreviation after\r\n",
    "                # converting it to uppercase (so lookup is case-insensitive)\r\n",
    "                return self._abbreviation_lookup[residue.upper()]\r\n",
    "        except AttributeError:\r\n",
    "                # if residue cannot be converted to uppercase, it is not a\r\n",
    "                # string, so raise an error\r\n",
    "                raise MutationError('Residue must be a string')\r\n",
    "        except KeyError:\r\n",
    "                # if residue is not a key in self._abbreviation_lookup, it\r\n",
    "                # it is not a standard amino acid residue, so raise an error\r\n",
    "                raise MutationError(\\\r\n",
    "                 'Input residue not recognized, must be a standard residue: '\\\r\n",
    "                  + residue)\r\n",
    "\r\n",
    "    def _get_wt_residue(self):\r\n",
    "        return self.__wt_residue\r\n",
    "    WtResidue = property(_get_wt_residue)\r\n",
    "\r\n",
    "    def _get_mut_residue(self):\r\n",
    "        return self.__mut_residue\r\n",
    "    MutResidue = property(_get_mut_residue)\r\n",
    "\r\n",
    "    def _get_original_mention(self):\r\n",
    "        self.__original_mention = self.__original_mention.strip()\r\n",
    "        raw_mut = self.__original_mention[1:] if not self.__original_mention[0].isalnum() else self.__original_mention\r\n",
    "        raw_mut = self.__original_mention[:-1] if not self.__original_mention[-1].isalnum() else self.__original_mention\r\n",
    "        return raw_mut.strip()\r\n",
    "    OriginalMention = property(_get_original_mention)\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        \"\"\" Return original mutation snippet\"\"\"\r\n",
    "        return ''.join([self.__wt_residue,str(self.Position),\r\n",
    "          self.__mut_residue])\r\n",
    "\r\n",
    "    def __eq__(self, other):\r\n",
    "        \"\"\" Override ==\r\n",
    "\r\n",
    "            Two PointMutation objects are equal if their Position, WtResidue,\r\n",
    "             and MutResidue values are all equal.\r\n",
    "        \"\"\"\r\n",
    "        if type(self) == type(other):\r\n",
    "          return self.Position == other.Position and \\\r\n",
    "                 self.__wt_residue == other.__wt_residue and \\\r\n",
    "                 self.__mut_residue == other.__mut_residue\r\n",
    "        return False\r\n",
    "\r\n",
    "    def __ne__(self,other):\r\n",
    "        \"\"\" Override !=\r\n",
    "\r\n",
    "            Two PointMutation obects are not equal if either their Position,\r\n",
    "             WtResidue, or MutResidue values differ.\r\n",
    "        \"\"\"\r\n",
    "        return not self == other\r\n",
    "\r\n",
    "    def __hash__(self):\r\n",
    "        \"\"\" Override hash() \"\"\"\r\n",
    "        return hash(str(type(self)) + str(self))\r\n",
    "\r\n",
    "#######\r\n",
    "\r\n",
    "class MutationExtractor(object):\r\n",
    "    \"\"\" A base class for extracting Mutations from text \"\"\"\r\n",
    "\r\n",
    "    def __init__(self,ignorecase=True):\r\n",
    "        \"\"\" Initialize the object \"\"\"\r\n",
    "        pass\r\n",
    "\r\n",
    "class MutationFinder(MutationExtractor):\r\n",
    "\r\n",
    "    def __init__(self,regular_expressions):\r\n",
    "        \"\"\" Initialize the object\r\n",
    "\r\n",
    "            regular_expressions: an interative set of regular expressions to\r\n",
    "                be applied for extracting mutations. These are in the\r\n",
    "                default python syntax (i.e., perl regular expressions), with\r\n",
    "                the single exception being that regular expressions which\r\n",
    "                should be performed in a case sensitive manner should be\r\n",
    "                followed by the string '[CASE_SENSITIVE]', with no spaces\r\n",
    "                between it and the regular expression.\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        MutationExtractor.__init__(self)\r\n",
    "        self._regular_expressions = []\r\n",
    "\r\n",
    "        for regular_expression in regular_expressions:\r\n",
    "            if regular_expression.endswith('[CASE_SENSITIVE]'):\r\n",
    "                self._regular_expressions.append(\\\r\n",
    "                 re.compile(regular_expression[:regular_expression.rindex('[')]))\r\n",
    "            else:\r\n",
    "                self._regular_expressions.append(\\\r\n",
    "                 re.compile(regular_expression,re.IGNORECASE))\r\n",
    "\r\n",
    "    def _post_process(self,mutations):\r\n",
    "        \"\"\" Perform precision increasing post-processing steps\r\n",
    "\r\n",
    "            Remove false positives indicated by:\r\n",
    "              -> mutant and wild-type residues being identical (e.g. A42A)\r\n",
    "        \"\"\"\r\n",
    "        for mutation in list(mutations):\r\n",
    "            if type(mutation) is PointMutation:\r\n",
    "                if mutation.WtResidue == mutation.MutResidue:\r\n",
    "                    del mutations[mutation]\r\n",
    "\r\n",
    "    def __call__(self,raw_text, span_size=100):\r\n",
    "        \"\"\" Extract point mutations mentions from raw_text and return them in a dict\r\n",
    "             raw_text: a string of text\r\n",
    "\r\n",
    "            The result of this method is a dict mapping PointMutation objects to\r\n",
    "             a list of spans where they were identified. Spans are presented in the\r\n",
    "             form of character-offsets in text. If counts for each mention are\r\n",
    "             required instead of spans, apply len() to each value to convert the\r\n",
    "             list of spans to a count.\r\n",
    "\r\n",
    "            Example result:\r\n",
    "             raw_text: 'We constructed A42G and L22G, and crystalized A42G.'\r\n",
    "             result = {PointMutation(42,'A','G'):[(15,19),(46,50)],\r\n",
    "                       PointMutation(22,'L','G'):[(24,28)]}\r\n",
    "\r\n",
    "             Note that the spans won't necessarily be in increasing order, due\r\n",
    "              to the order of processing regular expressions.\r\n",
    "\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        result = {}\r\n",
    "        for regular_expression in self._regular_expressions:\r\n",
    "            for m in regular_expression.finditer(raw_text):\r\n",
    "\r\n",
    "                span = min(m.span('wt_res')[0],\\\r\n",
    "                        m.span('pos')[0],\\\r\n",
    "                        m.span('mut_res')[0]),\\\r\n",
    "                    max(m.span('wt_res')[1],\\\r\n",
    "                        m.span('pos')[1],\\\r\n",
    "                        m.span('mut_res')[1])\r\n",
    "\r\n",
    "                current_mutation = \\\r\n",
    "                PointMutation(m.group('pos'),m.group('wt_res'),\\\r\n",
    "                            m.group('mut_res'), raw_text[span[0]:span[1]+1])\r\n",
    "\r\n",
    "                surrounding_text = raw_text[max(span[0]-span_size, 0):\\\r\n",
    "                                      min(len(raw_text), span[1]+span_size)]\r\n",
    "\r\n",
    "                if current_mutation not in result.keys():\r\n",
    "                  result[current_mutation] = surrounding_text\r\n",
    "\r\n",
    "        self._post_process(result)\r\n",
    "        return result\r\n",
    "\r\n",
    "\r\n",
    "def mutation_finder_from_regex_filepath(regular_expression_filepath):\r\n",
    "    \"\"\" \r\n",
    "    Constructs a MutationFinder object using regular expressions in a file\r\n",
    "    \"\"\"\r\n",
    "    regular_expressions_file = open(regular_expression_filepath)\r\n",
    "\r\n",
    "    regular_expressions = []\r\n",
    "    # Read in and store regular expression, ignoring lines that begin with '#'\r\n",
    "    for line in regular_expressions_file:\r\n",
    "        line = line.strip()\r\n",
    "        if not line.startswith('#'):\r\n",
    "            regular_expressions.append(line)\r\n",
    "\r\n",
    "    return MutationFinder(regular_expressions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "mf_regex_path = 'data/regexs/mutationfinder_regex/seth_modified.txt'\r\n",
    "mf_mut_extract = mutation_finder_from_regex_filepath(mf_regex_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text = 'A(1154)C'\r\n",
    "for mutation, snip in mf_mut_extract(raw_text=text, span_size=150).items():\r\n",
    "    mutation_entry = snip + ' : ' + mutation.OriginalMention\r\n",
    "    print(mutation_entry)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A(1154)C : A(1154)C\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 tmVar [Link](https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/tmvar/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class TmVar:\r\n",
    "    def __init__(self, regex_folder):\r\n",
    "        \"\"\" \r\n",
    "        regex_folder should contain the 4 regex files\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        self._regular_expressions = []\r\n",
    "\r\n",
    "        regular_expressions_file = open(os.path.join(regex_folder, 'MF.RegEx.2.txt'))\r\n",
    "        for line in regular_expressions_file:\r\n",
    "            reg, group = line.split('\\t')\r\n",
    "            # some regex in DNAMutation group might bring tons of FP\r\n",
    "            # but switching these off didn't have any immediate affect on test data during dev\r\n",
    "            # manual filtering required?\r\n",
    "            if not reg.startswith('#'):\r\n",
    "                if group == 'DNAMutation':\r\n",
    "                    reg = '[^0-9A-Za-z]' + reg + ' '\r\n",
    "                else:\r\n",
    "                    reg = '[^0-9A-Za-z]' + reg + '[^0-9A-Za-z]'\r\n",
    "                self._regular_expressions.append(re.compile(reg))\r\n",
    "\r\n",
    "        regular_expressions_file = open(os.path.join(regex_folder, 'SNP.RegEx.txt'))\r\n",
    "        for reg in regular_expressions_file:\r\n",
    "            reg = '[^0-9A-Za-z]' + reg +'[^0-9A-Za-z]'\r\n",
    "            self._regular_expressions.append(re.compile(reg))\r\n",
    "\r\n",
    "        regular_expressions_file = open(os.path.join(regex_folder, 'ProteinMutation.RegEx.txt'))\r\n",
    "        for reg in regular_expressions_file:\r\n",
    "            reg = '[^0-9A-Za-z]' + reg + '[^0-9A-Za-z]'\r\n",
    "            self._regular_expressions.append(re.compile(reg))\r\n",
    "\r\n",
    "        regular_expressions_file = open(os.path.join(regex_folder, 'DNAMutation.RegEx.txt'))\r\n",
    "        for reg in regular_expressions_file:\r\n",
    "            reg = '[^0-9A-Za-z]' + reg + '[^0-9A-Za-z]'\r\n",
    "            self._regular_expressions.append(re.compile(reg))\r\n",
    "\r\n",
    "    def __call__(self, text, span_size=150):\r\n",
    "        final_list = []\r\n",
    "        for regex in self._regular_expressions:       \r\n",
    "            for m in regex.finditer(text):\r\n",
    "                span = (m.start(0), m.end(0))\r\n",
    "                # have to post process to remove mutant and wild-type residues being identical (e.g. A42A)\r\n",
    "                # no quick way to do it tho - naive way would be manually edit the regex with WRES and MRES like in MF regex\r\n",
    "                # TODO: manual work time? :(                \r\n",
    "                surrounding_text = (text[max(span[0]-span_size, 0):\\\r\n",
    "                                        min(len(text), span[1]+span_size)])\r\n",
    "                raw_mut = (text[span[0]:span[1]]).strip()\r\n",
    "                raw_mut = raw_mut[1:] if not raw_mut[0].isalnum() else raw_mut\r\n",
    "                raw_mut = raw_mut[:-1] if not raw_mut[-1].isalnum() else raw_mut\r\n",
    "                final_list.append([raw_mut.strip(), surrounding_text])\r\n",
    "        return final_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tmvar_mut_extract = TmVar('data/regexs/tmvar_regex/final_regex_path')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_16249/1451537365.py:20: FutureWarning: Possible nested set at position 15\n",
      "  self._regular_expressions.append(re.compile(reg))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tmvar_mut_extract(' n2923 (A347V), n2870(R429K), and n1163(S486F')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['A347V', ' n2923 (A347V), n2870(R429K), and n1163(S486F'],\n",
       " ['R429K', ' n2923 (A347V), n2870(R429K), and n1163(S486F']]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Extra custom regexs "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from wbtools.db.generic import WBGenericDBManager\r\n",
    "from wbtools.db.gene import WBGeneDBManager\r\n",
    "from wbtools.lib.nlp.common import EntityType\r\n",
    "\r\n",
    "import configparser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "db_config = configparser.ConfigParser()\r\n",
    "db_config.read('utils/all_config.cfg')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['utils/all_config.cfg']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "if not set(os.listdir('data/gsoc/wbtools')) >= set(['wb_alleles_variations.npy', 'wb_allele_designations.npy', 'all_gene_names.npy']):\r\n",
    "                                            \r\n",
    "    db_manager = WBGenericDBManager(\r\n",
    "        dbname=db_config['wb_database']['db_name'], user=db_config['wb_database']['db_user'],\r\n",
    "        password=db_config['wb_database']['db_password'], host=db_config['wb_database']['db_host'])\r\n",
    "\r\n",
    "    alleles_variations = db_manager.get_curated_entities(entity_type=EntityType.VARIATION, exclude_id_used_as_name=False)\r\n",
    "    allele_designations = db_manager.get_allele_designations()\r\n",
    "\r\n",
    "    db_manager = WBGeneDBManager(dbname=db_config['wb_database']['db_name'], user=db_config['wb_database']['db_user'],\\\r\n",
    "                                    password=db_config['wb_database']['db_password'], host=db_config['wb_database']['db_host'])\r\n",
    "\r\n",
    "    all_gene_names = db_manager.get_all_gene_names()\r\n",
    "    genes = []\r\n",
    "    for gene in all_gene_names.values():\r\n",
    "        if gene:\r\n",
    "            genes.append(gene[0])\r\n",
    "\r\n",
    "    np.save('data/gsoc/wbtools/wb_alleles_variations.npy', alleles_variations)\r\n",
    "    np.save('data/gsoc/wbtools/wb_allele_designations.npy', allele_designations)\r\n",
    "    np.save('data/gsoc/wbtools/all_gene_names.npy', genes)\r\n",
    "    \r\n",
    "    # not sure if this line even releases any memory \r\n",
    "    alleles_variations = None\r\n",
    "    allele_designations = None\r\n",
    "    all_gene_names = None\r\n",
    "    genes = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class CustomWBregex:\r\n",
    "    def __init__(self):\r\n",
    "        OPENING_CLOSING_REGEXES = [r'(', r')']\r\n",
    "        wb_genes = np.load('data/gsoc/wbtools/all_gene_names.npy')\r\n",
    "        all_genes = Path('data/gsoc/wbtools/genes.txt').read_text().split('\\n')\r\n",
    "        for g in wb_genes: all_genes.append(g)\r\n",
    "        all_genes = [g for g in all_genes if len(g) > 1]\r\n",
    "        all_genes = list(set(all_genes))\r\n",
    "        all_genes = OPENING_CLOSING_REGEXES[0] + '|'.join(all_genes) + OPENING_CLOSING_REGEXES[1]\r\n",
    "\r\n",
    "        # the allele regex and db idea was stolen from wbtools\r\n",
    "        allele_designations = np.load('data/gsoc/wbtools/wb_allele_designations.npy').astype('U6')\r\n",
    "        alleles_variations = np.load('data/gsoc/wbtools/wb_alleles_variations.npy').astype('U6')\r\n",
    "        DB_VAR_REGEX = r'({designations}|m|p|ts|gf|lf|d|sd|am|cs)([0-9]+)'\r\n",
    "        var_regex_1 = OPENING_CLOSING_REGEXES[0] + DB_VAR_REGEX.format(designations=\"|\".join(allele_designations)) + OPENING_CLOSING_REGEXES[1]\r\n",
    "        all_var = OPENING_CLOSING_REGEXES[0] + '|'.join(alleles_variations) + '|' + var_regex_1 + OPENING_CLOSING_REGEXES[1]\r\n",
    "\r\n",
    "        variation_regex = [\\\r\n",
    "            all_var + r'[^A-Za-z].*[^A-Za-z](bp|base pair).*([ACTG]{8,}).*([ACTG]{8,})',\\\r\n",
    "            all_var + r'[^A-Za-z].{,50}(deletes|deletion|inserts|insertion).{,50}[^A-Za-z](bp|base pair).*(flank)',\\\r\n",
    "            all_var + r'[^A-Za-z].{,50}(deletes|deletion|inserts|insertion).{,50}(exon|intron) +[0-9]+',\\\r\n",
    "            all_var + r'[^A-Za-z].{,50}[^A-Za-z](bp|base pair).{,50}(deletes|deletion|inserts|insertion)',\\\r\n",
    "            all_var + r'[^A-Za-z].*( [CISQMNPKDTFAGHLRWVEYBZJX]) *(?: *-*> *| +(in|to|into|for|of|by|with|at)) +(either +)?((an|a) +)?( *NONSENSE +)?(TERM|STOP|AMBER|OCHRE|OPAL|UMBER)',\\\r\n",
    "            all_var + r'[^A-Za-z].*( [CISQMNPKDTFAGHLRWVEYBZJX]) *(?: *-*> *| +(in|to|into|for|of|by|with|at)) +(either +)?((an|a) +)?( *NONSENSE +)?([CISQMNPKDTFAGHLRWVEYBZJX*][^0-9A-Za-z])',\\\r\n",
    "            ]\r\n",
    "        gene_var_combo = [\\\r\n",
    "            all_var + r'[^A-Za-z]{0,2}' + all_genes + r'[^A-Za-z]',\\\r\n",
    "            all_genes + r'[^A-Za-z]{0,2}' + all_var + r'[^A-Za-z]',\r\n",
    "            ]\r\n",
    "\r\n",
    "        self._gene_var_regex = [re.compile(r,re.IGNORECASE) for r in gene_var_combo]\r\n",
    "\r\n",
    "        # these regexes were written after manually looking at the curator remarks\r\n",
    "        raw_regexs = [\\\r\n",
    "            '(?:^|[\\s\\(\\[\\'\"/,;\\-])([CISQMNPKDTFAGHLRWVEYBZJX]) *(\\(?[1-9][0-9]*\\)?)(?: *-*> *| +(in|to|into|for|of|by|with|at))? +(either +)?((an|a) +)?( *NONSENSE +)?(TERM|STOP|AMBER|OCHRE|OPAL|UMBER)',\\\r\n",
    "            '(?:^|[\\s\\(\\[\\'\"/,;\\-])([CISQMNPKDTFAGHLRWVEYBZJX])(?: *-*> *| +(in|to|into|for|of|by|with|at)) +(either +)?((an|a) +)?( *NONSENSE +)?(TERM|STOP|AMBER|OCHRE|OPAL|UMBER)',\\\r\n",
    "            '(?:^|[\\s\\(\\[\\'\"/,;\\-])([CISQMNPKDTFAGHLRWVEYBZJX])(?: *-*> *| +(in|to|into|for|of|by|with|at) +(either +)?((an|a) +)?)( *NONSENSE +)?([CISQMNPKDTFAGHLRWVEYBZJX*])[^0-9A-Za-z].*(flank)',\\\r\n",
    "            '(?:^|[\\s\\(\\[\\'\"/,;\\-])([CISQMNPKDTFAGHLRWVEYBZJX])(?: *-*> *| +(in|to|into|for|of|by|with|at) +(either +)?((an|a) +)?)( *NONSENSE +)?([CISQMNPKDTFAGHLRWVEYBZJX*])[^0-9A-Za-z].*([ACTG]{8,}).*([ACTG]{8,})',\\\r\n",
    "            '(?:^|[\\s\\(\\[\\'\"/,;\\-])(flank).*([ACTG]{8,}).*([ACTG]{8,})'\r\n",
    "            ]\r\n",
    "\r\n",
    "        self._regular_expressions = [re.compile(r,re.IGNORECASE) for r in raw_regexs + variation_regex]\r\n",
    "\r\n",
    "    def __call__(self, text, span_size=150):\r\n",
    "        final_list = []\r\n",
    "        for regex in self._regular_expressions:      \r\n",
    "            for m in regex.finditer(text):\r\n",
    "                span = (m.start(0), m.end(0))    \r\n",
    "                surrounding_text = (text[max(span[0]-span_size, 0):\\\r\n",
    "                                        min(len(text), span[1]+span_size)])\r\n",
    "                raw_mut = (text[span[0]:span[1]])\r\n",
    "                raw_mut = raw_mut[1:] if not raw_mut[0].isalnum() else raw_mut\r\n",
    "                raw_mut = raw_mut[:-1] if not raw_mut[-1].isalnum() else raw_mut\r\n",
    "                final_list.append([raw_mut.strip(), surrounding_text])\r\n",
    "\r\n",
    "        return final_list\r\n",
    "\r\n",
    "    def var_and_gene_close(self, text, span_size=150):\r\n",
    "        final_list = []\r\n",
    "        for regex in self._gene_var_regex:      \r\n",
    "            for m in regex.finditer(text):\r\n",
    "                span = (m.start(0), m.end(0))    \r\n",
    "                surrounding_text = (text[max(span[0]-span_size, 0):\\\r\n",
    "                                        min(len(text), span[1]+span_size)])\r\n",
    "                raw_mut = (text[span[0]:span[1]])\r\n",
    "                raw_mut = raw_mut[1:] if not raw_mut[0].isalnum() else raw_mut\r\n",
    "                raw_mut = raw_mut[:-1] if not raw_mut[-1].isalnum() else raw_mut\r\n",
    "                final_list.append([raw_mut.strip(), 'Gene & Variant'])\r\n",
    "\r\n",
    "        return final_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "custom_mut_extract = CustomWBregex()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- in the statement ced-3(n2888) \r\n",
    "n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "custom_mut_extract('n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT',\n",
       "  'n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT']]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Bag of words "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class BOWdictionary:\r\n",
    "    def __init__(self):\r\n",
    "        # words whose presence would automatically tick sentence posititve without any context\r\n",
    "        self.list_of_words = [\\\r\n",
    "            ['substitution', 'downstream', 'deletion', 'frameshift'],\\\r\n",
    "            ]\r\n",
    "        \r\n",
    "    @staticmethod \r\n",
    "    def tokenize_string(string):\r\n",
    "        sentence = string\r\n",
    "        sentence = re.sub('([0-9])([A-Za-z])', r'\\1 \\2', sentence)\r\n",
    "        # separate non-ascii characters into their own tokens\r\n",
    "        sentence = re.sub('([^\\x00-\\x7F])', r' \\1 ', sentence)\r\n",
    "        sentence = re.sub('([\\W\\-_])', r' \\1 ', sentence)\r\n",
    "        return sentence.split()  # splits by white space\r\n",
    "\r\n",
    "    def __call__(self, text):\r\n",
    "        final_list = []\r\n",
    "        for single_list in self.list_of_words:\r\n",
    "            word_set = set(single_list)\r\n",
    "            phrase_set = set(BOWdictionary.tokenize_string(text))\r\n",
    "            if phrase_set >= word_set:\r\n",
    "                final_list.append(['Invalid', text])\r\n",
    "        return final_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "bow_mut_extract = BOWdictionary()\r\n",
    "# bow_mut_extract('This mutation deletes 471bp of the promoter region, the transcriptional start and 56 amino acids of the second exon.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 MF + tmVar + Custom regex + BOW"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def unique_rows(a):\r\n",
    "    a = np.ascontiguousarray(a)\r\n",
    "    unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\r\n",
    "    return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))\r\n",
    "\r\n",
    "\r\n",
    "def regex_block(sentence, span_size=150):\r\n",
    "    mut_and_snippets = []\r\n",
    "    \r\n",
    "    # MutationFinder\r\n",
    "    for mutation, snip in mf_mut_extract(raw_text=sentence, span_size=span_size).items():\r\n",
    "        mut_and_snippets.append([mutation.OriginalMention, snip])\r\n",
    "    \r\n",
    "    # tmVar\r\n",
    "    mut_and_snippets = mut_and_snippets + tmvar_mut_extract(sentence, span_size=span_size)\r\n",
    "    # Custom patterns\r\n",
    "    mut_and_snippets = mut_and_snippets + custom_mut_extract(sentence, span_size=span_size)\r\n",
    "    # Bag of words\r\n",
    "    mut_and_snippets = mut_and_snippets + bow_mut_extract(sentence)\r\n",
    "    \r\n",
    "    if mut_and_snippets:\r\n",
    "        mut_and_snippets = unique_rows(mut_and_snippets).tolist()\r\n",
    "    return mut_and_snippets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "regex_block(' asdf gpa-2 ::Tc1 asdf as')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7 * Additional details  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def extra_info_block(sentence, span_size=150):\r\n",
    "    info_and_snippets = []\r\n",
    "\r\n",
    "    # look for gene and variant combo\r\n",
    "    info_and_snippets = info_and_snippets + custom_mut_extract.var_and_gene_close(sentence, span_size=span_size)\r\n",
    "    \r\n",
    "    if info_and_snippets:\r\n",
    "        info_and_snippets = unique_rows(info_and_snippets).tolist()\r\n",
    "    return info_and_snippets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "extra_info_block('in the statement ced-3(n2888)')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['ced-3(n2888', 'Gene & Variant']]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Changes in scores\r\n",
    "MF + tmVar only: on remarks - (0.9459459459459459, 0.39923954372623577, 0.5614973262032086, None)   \r\n",
    "on remarks + nala - (0.8671477079796265, 0.6698360655737705, 0.755826859045505, None)  \r\n",
    "    \r\n",
    "MF + tmVar + Custom + BOW: on remarks - (0.9606741573033708, 0.6501901140684411, 0.7755102040816326, None)  \r\n",
    "on remarks + nala - (0.8680868496517821, 0.6947540983606557, 0.7718084137679839, None)  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 BioBERT NER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import torch\r\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\r\n",
    "from transformers import TokenClassificationPipeline\r\n",
    "\r\n",
    "import re\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import sklearn as sk\r\n",
    "import math \r\n",
    "import string\r\n",
    "import time\r\n",
    "import json\r\n",
    "import csv\r\n",
    "import shutil\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/risubu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model_name_or_path = 'models/nala'\r\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n",
    "model = AutoModelForTokenClassification.from_pretrained(\r\n",
    "    model_name_or_path,\r\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\r\n",
    "    config=config,\r\n",
    ")\r\n",
    "# LABEL_0 - B-mut, LABEL_1 - I-mut, LABEL_2 - O\r\n",
    "nala_ner  = TokenClassificationPipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='first')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\r\n",
    "stop_words = [w for w in stop_words if len(w) > 1]\r\n",
    "\r\n",
    "def ner_score(sentence):\r\n",
    "    mutations = []\r\n",
    "    try:\r\n",
    "        ner_output = nala_ner(sentence)\r\n",
    "        for i, grp in enumerate(ner_output):\r\n",
    "            if grp['entity_group'] == 'LABEL_0':\r\n",
    "                mut = grp['word']\r\n",
    "                for j in range(i+1, len(ner_output)):\r\n",
    "                    if ner_output[j]['entity_group'] == 'LABEL_1':\r\n",
    "                        mut  = mut + ' ' + ner_output[j]['word']\r\n",
    "                    else:\r\n",
    "                        # NER would be handling only data in NL form\r\n",
    "                        if len(mut.split()) > 3 and any(word in mut.split() for word in stop_words):\r\n",
    "                            mutations.append([mut, sentence])\r\n",
    "                        break\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    return mutations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Testing \r\n",
    "## 3.2 On scored curator remarks and nala mutation corpus "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# all_texts = []\r\n",
    "# issues_count = 0\r\n",
    "\r\n",
    "# df = pd.read_csv(r\"data\\gsoc\\Remarks_scored.csv\")\r\n",
    "# df = df.to_numpy()\r\n",
    "# # extract sentences\r\n",
    "# text = []\r\n",
    "# y_true = []\r\n",
    "# for idx, row in enumerate(df):\r\n",
    "#     loc = str(row[2]).find('Paper_evidence')\r\n",
    "#     if loc != -1:\r\n",
    "#         if row[0].split()[0] == 'Yes':\r\n",
    "#             y_true.append(1)\r\n",
    "#         elif row[0].split()[0] == 'No':\r\n",
    "#             y_true.append(0)\r\n",
    "#         else:\r\n",
    "#             continue\r\n",
    "#         temp_str = str(row[2][1:loc-2]).replace(\"\\\"\", \"'\")\r\n",
    "#         text.append(temp_str)\r\n",
    "\r\n",
    "# assert len(text) == len(y_true)\r\n",
    "# print('Count from Remarks_scored = {}'.format(len(text)))\r\n",
    "\r\n",
    "# y_pred = []\r\n",
    "# for sentence in text:\r\n",
    "#     if regex_block(sentence):\r\n",
    "#         y_pred.append(1)\r\n",
    "#     elif ner_score(sentence):\r\n",
    "#         y_pred.append(1)\r\n",
    "#     else:\r\n",
    "#         y_pred.append(0)\r\n",
    "\r\n",
    "# assert len(y_pred) == len(y_true)\r\n",
    "# all_texts = all_texts + text\r\n",
    "\r\n",
    "\r\n",
    "# df = pd.read_csv(r\"data\\nala\\binary_nala_NOT_NER.csv\")\r\n",
    "# df = df.to_numpy()\r\n",
    "# print('Count from nala = {}'.format(len(df)))\r\n",
    "\r\n",
    "# print('Entries processed: ', end=' ')\r\n",
    "# for i, row in enumerate(df):\r\n",
    "#     if i%500 == 0: print(i, end=' ')\r\n",
    "#     sentence = row[0]\r\n",
    "#     label = row[1]\r\n",
    "#     try:\r\n",
    "#         if regex_block(sentence):\r\n",
    "#             y_pred.append(1)\r\n",
    "#         elif ner_score(sentence):\r\n",
    "#             y_pred.append(1)\r\n",
    "#         else:\r\n",
    "#             y_pred.append(0)\r\n",
    "\r\n",
    "#         if label == 1:\r\n",
    "#             y_true.append(1)\r\n",
    "#         else:\r\n",
    "#             y_true.append(0)\r\n",
    "        \r\n",
    "#         all_texts.append(sentence)\r\n",
    "#     except:\r\n",
    "#         issues_count += 1\r\n",
    "#         pass\r\n",
    "\r\n",
    "# assert len(y_pred) == len(y_true) == len(all_texts)\r\n",
    "# print('\\nTotal count = {}'.format(len(y_pred)))\r\n",
    "# if issues_count: print('Note: Could not process {} sentences'.format(issues_count))\r\n",
    "\r\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='binary')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# # Manually inspecting incorrect preds\r\n",
    "# bad = []\r\n",
    "# assert len(y_pred) == len(y_true) == len(all_texts)\r\n",
    "# for i, (t, p,sent) in enumerate(zip(y_true, y_pred, all_texts)):\r\n",
    "#     if t != p:\r\n",
    "#         if t:\r\n",
    "#             bad.append([sent,i])\r\n",
    "# len(bad)\r\n",
    "# # print(bad[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Changes in scores (find the test cell block below)\r\n",
    "  \r\n",
    "Regex only: on remarks - (0.9606741573033708, 0.6501901140684411, 0.7755102040816326, None)  \r\n",
    "on remarks + nala - (0.8680868496517821, 0.6947540983606557, 0.7718084137679839, None) \r\n",
    "  \r\n",
    "Regex + NER: on remarks - (0.8592057761732852, 0.9049429657794676, 0.8814814814814815, None)   \r\n",
    "on remarks + nala - (0.7570694087403599, 0.9655737704918033, 0.8487031700288185, None)  \r\n",
    "  \r\n",
    "### Retraining on NL and SST  \r\n",
    "Regex + NER: on remarks - (0.9627906976744186, 0.7870722433460076, 0.8661087866108786, None)      \r\n",
    "on remarks + nala - (0.8461816865725661, 0.7665573770491804, 0.804403922243248, None)  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 On WB papers\r\n",
    "### Get the paper texts from textpresso API and wbtools"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How it works - First paper ID is searched through textpresso API. If the recieved output is blank, then wbtools is used.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import sys\r\n",
    "import os.path\r\n",
    "import argparse\r\n",
    "import subprocess\r\n",
    "import requests\r\n",
    "import csv\r\n",
    "import json\r\n",
    "import nltk.data\r\n",
    "import os\r\n",
    "from xml.dom import minidom\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from wbtools.literature.corpus import CorpusManager\r\n",
    "import platform"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uncomment and run the cell below only once to save the paper sentences in your local computer to avoid pulling paper text everytime during dev  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Create a separate temporary numpy file to store the paper sentences  \r\n",
    "to avoid spending time on pulling sentences while in development \r\n",
    "Format - [paper_id, sentence]\r\n",
    "'''\r\n",
    "# final_data = []\r\n",
    "# remove_sections = []\r\n",
    "# # random 100 papers mentioned the remarks ace file in data/gsoc\r\n",
    "# paper_ids = np.load('data\\top100.npy')\r\n",
    "# for i, paper_id in enumerate(paper_ids):\r\n",
    "#     paper_id = paper_id[7:]\r\n",
    "#     cm.load_from_wb_database(db_name=config['wb_database']['db_name'], db_user=config['wb_database']['db_user'], db_password=config['wb_database']['db_password'],\r\n",
    "#         db_host=config['wb_database']['db_host'], paper_ids=[paper_id],\r\n",
    "#         ssh_host=config['wb_database']['ssh_host'], ssh_user=config['wb_database']['ssh_user'], ssh_passwd=config['wb_database']['ssh_passwd'],\r\n",
    "#         load_bib_info=False, load_afp_info=False, load_curation_info=False)\r\n",
    "#     sentences = cm.get_paper(paper_id).get_text_docs(remove_sections=remove_sections,split_sentences=True)\r\n",
    "#     for sent in sentences:\r\n",
    "#       final_data.append([paper_id, sent])\r\n",
    "#     print(i, end = \" \")\r\n",
    "# final_data = np.array(final_data)\r\n",
    "# np.save('id_and_sentence.npy', final_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def textpresso_paper_text(wbpid, path, token):\r\n",
    "    \"\"\"This sub takes a wbpid eg WBPaper00056731 and returns the fulltext paper in sentences\"\"\"\r\n",
    "    \r\n",
    "    ft=[0];\r\n",
    "    # Check that wbpid is a valid WBPaper\r\n",
    "    if not re.match( 'WBPaper', wbpid):\r\n",
    "        print (wbpid, \"is not a valid WBPaper ID\")\r\n",
    "        return ft\r\n",
    "    # Download paper if it doesn't exist\r\n",
    "    fn = path + '/temp/' + wbpid + '.json'\r\n",
    "\r\n",
    "    if os.path.exists(fn) and os.path.getsize(fn) > 16:\r\n",
    "        pass\r\n",
    "    else:\r\n",
    "        com1 = '-o '+fn +'\\n-k '+ '\\n'+'-d \"{\\\\\"token\\\\\":\\\\\"'+ token + '\\\\\", \\\\\"query\\\\\": {\\\\\"accession\\\\\": \\\\\"' + wbpid +'\\\\\", \\\\\"type\\\\\": \\\\\"document\\\\\", \\\\\"corpora\\\\\": [\\\\\"C. elegans\\\\\"]}, \\\\\"include_fulltext\\\\\": true}\"'\r\n",
    "        configf= path +'/temp/' + wbpid + '.tmp.config'\r\n",
    "        curlf = open(configf,'w')\r\n",
    "        print (com1, file=curlf)\r\n",
    "        curlf.close()\r\n",
    "        command = 'curl -o '+ fn +' -K '+ configf+' https://textpressocentral.org:18080/v1/textpresso/api/search_documents' \r\n",
    "        comlist = command.split()\r\n",
    "        os.system(command)\r\n",
    "\r\n",
    "    # Read the paper, and split into sentences\r\n",
    "    if os.path.exists(fn) and os.path.getsize(fn) > 20:\r\n",
    "        # Open our JSON file and load it into python\r\n",
    "        input_file = open (fn)\r\n",
    "        json_array = json.load(input_file)\r\n",
    "        for item in json_array:\r\n",
    "            abs = item[\"abstract\"]\r\n",
    "            fullt =  item[\"fulltext\"]\r\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
    "            ft = tokenizer.tokenize(abs)\r\n",
    "            ftt=tokenizer.tokenize(fullt)\r\n",
    "            ft = ft +ftt\r\n",
    "    else:\r\n",
    "        # some paper texts are blank for some reason\r\n",
    "        # pipeline uses wbtools to get sentences in such case\r\n",
    "        pass\r\n",
    "\r\n",
    "    outfilen = os.path.join(path, 'text_flatfiles', wbpid+'.txt')\r\n",
    "    outf = open(outfilen, 'w')\r\n",
    "    for sen in ft:\r\n",
    "        sen =str(sen)\r\n",
    "        print(sen, file=outf)\r\n",
    "    outf.close()\r\n",
    "    return outfilen\r\n",
    "\r\n",
    "\r\n",
    "'''\r\n",
    "Sentences out of wbtools are sometimes weird, especially for the old papers\r\n",
    "This increases the false neg rate so this is used only when the textpresso api provides bad text\r\n",
    "Most of the issues have band aid fixes in the text preprocessing cell below but there are some \r\n",
    "with no easy fix or need to be worked on - \r\n",
    "1. Table content is extracted column wise, not row wise \r\n",
    "2. Some sentences have white space between every character and are also somehow inverted (???)\r\n",
    "    e.g. Check out the sentences of WBPaper00002018 (1994)\r\n",
    "    Line 154 -  '0 7 1 u ( 7 - c e m    ) F (   .'\r\n",
    "    inverted and without the white space is (mec-7)u170 which is extremely useful and will get missed \r\n",
    "    by the pipeline unless processed correctly.\r\n",
    "TODO: Not sure how to solve point 1 but point 2 is easy to solve and also helps a LOT.\r\n",
    "    Rishab, work on this after you complete the project. \r\n",
    "    Not high priority as this might be only for the >10 year old papers (which are already manually curated)\r\n",
    "'''\r\n",
    "cm = CorpusManager()\r\n",
    "def wbtools_paper_text(wbpid, db_name, db_user, db_password, db_host, ssh_host,\\\r\n",
    "    ssh_user, ssh_passwd):\r\n",
    "    # sectioning might not be always correct, text processing is done separately in the pipeline\r\n",
    "    # remove_sections = [PaperSections.ACKNOWLEDGEMENTS, PaperSections.REFERENCES, PaperSections.RELATED_WORK, PaperSections.INTRODUCTION]\r\n",
    "    remove_sections = []\r\n",
    "    paper_id = wbpid[7:]\r\n",
    "    cm.load_from_wb_database(db_name=db_name, db_user=db_user, db_password=db_password,\r\n",
    "        db_host=db_host, paper_ids=[paper_id],\r\n",
    "        ssh_host=ssh_host, ssh_user=ssh_user, ssh_passwd=ssh_passwd,\r\n",
    "        load_bib_info=False, load_afp_info=False, load_curation_info=False)\r\n",
    "    sentences = cm.get_paper(paper_id).get_text_docs(remove_sections=remove_sections,split_sentences=True)\r\n",
    "    return sentences\r\n",
    "\r\n",
    "\r\n",
    "def get_paper_sentences(wbpids, config, store_ppr_path):\r\n",
    "    '''\r\n",
    "    Takes WB Paper IDs and returns a list of sentences from those papers after filtering\r\n",
    "    Arg:\r\n",
    "    wbpids - List of wb papers ids \r\n",
    "        e.g. ['WBPaper00002379']\r\n",
    "    config_path - Config file path\r\n",
    "    store_ppr_path - Folder path to store the paper flatfiles retrieved from TextPresso for future use\r\n",
    "    Returns:\r\n",
    "    paperid_sentence_list: List of paper ID and sentence\r\n",
    "        e.g. [['WBPaper00002379', 'First sentence'], ['WBPaper00002379', 'Second sentence'], ....]\r\n",
    "    '''\r\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\r\n",
    "    stop_words = [w for w in stop_words if len(w) > 1]\r\n",
    "\r\n",
    "    all_special_chars = []\r\n",
    "    with open('data/nala/train_dev.json') as f:\r\n",
    "        for jsonObj in f:\r\n",
    "            nala_json = json.loads(jsonObj)['tokens']\r\n",
    "            for word in nala_json:\r\n",
    "                if not word.isalnum():\r\n",
    "                    all_special_chars.append(word)\r\n",
    "    # list of special characters to keep during inference\r\n",
    "    # helps with clearing out the bad characters from old papers\r\n",
    "    all_special_chars = list(set(all_special_chars))\r\n",
    "    \r\n",
    "    token = config['textpresso']['token']\r\n",
    "    db_name=config['wb_database']['db_name']\r\n",
    "    db_user=config['wb_database']['db_user']\r\n",
    "    db_password=config['wb_database']['db_password']\r\n",
    "    db_host=config['wb_database']['db_host']\r\n",
    "    ssh_host=config['wb_database']['ssh_host']\r\n",
    "    ssh_user=config['wb_database']['ssh_user']\r\n",
    "    ssh_passwd=config['wb_database']['ssh_passwd']\r\n",
    "\r\n",
    "    temp_paperid_sentence = np.array([])\r\n",
    "    if os.path.isfile('data/id_and_sentence.npy'):\r\n",
    "        temp_paperid_sentence = np.load('data/id_and_sentence.npy')\r\n",
    "    paperid_sentence_list = np.array([['WBPaperID', 'Sentence']])\r\n",
    "\r\n",
    "    for id in wbpids:\r\n",
    "        # textpresso_paper_text() also saves the text in flatfiles for future use \r\n",
    "        paper_path = textpresso_paper_text(id, store_ppr_path, token)\r\n",
    "        txt = Path(paper_path).read_text().split('\\n')\r\n",
    "        # deals with empty text files with only \"0\"\r\n",
    "        if len(txt) == 2:\r\n",
    "            if platform.system() != 'Windows':\r\n",
    "                txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\r\n",
    "                    ssh_user, ssh_passwd)\r\n",
    "            elif temp_paperid_sentence.size != 0:\r\n",
    "                txt = temp_paperid_sentence[temp_paperid_sentence[:, 0] == id[7:]][:, 1]\r\n",
    "            \r\n",
    "        for row in txt: \r\n",
    "            if row.find('fifi') == -1:\r\n",
    "                if platform.system() != 'Windows':\r\n",
    "                    txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\r\n",
    "                        ssh_user, ssh_passwd)\r\n",
    "                elif temp_paperid_sentence.size != 0:\r\n",
    "                    txt = temp_paperid_sentence[temp_paperid_sentence[:, 0] == id[7:]][:, 1]\r\n",
    "                break\r\n",
    "            \r\n",
    "        count_total_rows = len(txt)\r\n",
    "        for current_i, row in enumerate(txt):\r\n",
    "            if row.lower().find(\"we thank\") == 0 or row.lower().find(\"this work was supported\") == 0 \\\r\n",
    "                or row.lower().find(\"references\") == 0 or row.lower().find(\"we also thank\") == 0 \\\r\n",
    "                or row.lower().find(\"this research was supported\") == 0 or row.lower().find(\"we acknowledge\") == 0 \\\r\n",
    "                or row.lower().find(\"acknowledgments\") == 0 or row.lower().find('literature cited') != -1:\r\n",
    "                if current_i > count_total_rows/2:\r\n",
    "                    break\r\n",
    "\r\n",
    "            # usually is bad sentence\r\n",
    "            if len(row) < 40 or not any(word in row.lower().split() for word in stop_words):\r\n",
    "                continue\r\n",
    "            # remove sentences with links and email ids\r\n",
    "            if re.search('\\S+@\\S+\\.', row) or re.search('www.\\S+\\.', row):\r\n",
    "                continue\r\n",
    "            # filters one word sentences\r\n",
    "            if len(row.split()) == 1:\r\n",
    "                continue\r\n",
    "            # sentences comprised of only single characters \r\n",
    "            # ^ seems to be issue with wbtools extraction pipeline \r\n",
    "            if all(len(word) < 5 for word in row.split()):\r\n",
    "                continue\r\n",
    "            row = re.sub(\"\\( *cid *: *\\d+ *\\)\", \" \", row)\r\n",
    "            temp_row = row\r\n",
    "            for c in temp_row:\r\n",
    "                if (not c.isalnum() and not c == ' ') and c not in all_special_chars:\r\n",
    "                        row = row.replace(c, \"\")\r\n",
    "            # fixes bad space between each character of flanking sequence from old papers\r\n",
    "            flanking_regex = re.compile('([ACTG]( +)){4,}', re.IGNORECASE)\r\n",
    "            for m in flanking_regex.finditer(row):\r\n",
    "                span = (m.start(0), m.end(0))   \r\n",
    "                span = row[span[0]:span[1]-1]\r\n",
    "                correct_flank = re.sub('([ACTG])( +)', r'\\1', row, flags=re.I)\r\n",
    "                row = row.replace(span, correct_flank)\r\n",
    "\r\n",
    "            # filters out repeated lines, e.g. check out WBPaper00028727.txt in flatfiles folder\r\n",
    "            if row not in paperid_sentence_list[paperid_sentence_list[:,0]==id][:,1]:\r\n",
    "                paperid_sentence_list = np.vstack((paperid_sentence_list, [id, row]))\r\n",
    "    return paperid_sentence_list[1:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# papers mentioned the remarks ace file in data/gsoc\n",
    "ids_to_extract = np.load('data/top100.npy').tolist()[-2:]\n",
    "# ids_to_extract = ['WBPaper00002379']\n",
    "paperid_sentence_list = get_paper_sentences(ids_to_extract, db_config, store_ppr_path='data/wbpapers')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 33181  100 33033  100   148  12531     56  0:00:02  0:00:02 --:--:-- 12587\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 71684  100 71536  100   148  19513     40  0:00:03  0:00:03 --:--:-- 19553\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print('Number of sentences and characters: ', end=' ')\n",
    "print(len(paperid_sentence_list), sum([len(sent[1]) for sent in paperid_sentence_list]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of sentences and characters:  492 82594\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This cell takes a while to run - mainly due to the huge regex blocks   \r\n",
    "~ 2 seconds per sentence   \r\n",
    "Can't really switch them off though. There might be a smarter way to work with the regex block but this pipeline would be probably running every month on 50 or so papers (around 8 hours) so low priority.   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "final = [\n",
    "    ['temporary', 'temporary', 'temporary', 'temporary', 'temporary'],\\\n",
    "    ['WBPaper ID', 'Method', '*Gene-Variant combo', 'Mutation', 'Sentence']]\n",
    "total_sentences = len(paperid_sentence_list)\n",
    "print('Total sentences to process ', len(paperid_sentence_list))\n",
    "print('{total sentences processed}>{sentences with mutation}')\n",
    "for ppr_sen_idx, row in enumerate(paperid_sentence_list):\n",
    "    if (ppr_sen_idx+1) % 50 == 0: print(f\"{ppr_sen_idx+1}>{len(final)-1}\", end = \" \")\n",
    "    paper_id = row[0]\n",
    "    sentence = str()\n",
    "    limit = min(ppr_sen_idx+2, total_sentences)\n",
    "    # some sentences - mostly table content are super long\n",
    "    # temp fix, need to have a nice sentence splitter to minimize manual verification time\n",
    "    not_single_sentence = False\n",
    "    for i in range(ppr_sen_idx, limit):\n",
    "\n",
    "        sentence = sentence + paperid_sentence_list[i][1] + ' '\n",
    "\n",
    "        if (len(sentence) > 250 and not_single_sentence):\n",
    "            break\n",
    "        if paper_id != paperid_sentence_list[i][0]:\n",
    "            break\n",
    "        \n",
    "        var_plus_genes = ''\n",
    "        # Look for gene-variant combo e.g 'ced-3(n2888)' only on single sentences \n",
    "        if not not_single_sentence:\n",
    "            var_plus_genes = []\n",
    "            for data_and_cat in custom_mut_extract.var_and_gene_close(sentence.strip()):\n",
    "                var_plus_genes.append(data_and_cat[0])\n",
    "            if var_plus_genes:\n",
    "                var_plus_genes = \"'\" + \"', '\".join(var_plus_genes) + \"'\"\n",
    "            else:\n",
    "                var_plus_genes = ''\n",
    "                \n",
    "        output = regex_block(sentence.strip())\n",
    "        if output:\n",
    "            mutations = []\n",
    "            for mut_and_snip in output:\n",
    "                # temp fix to deal with same mutation getting detected due to stiching multiple sentences\n",
    "                if (mut_and_snip[0] not in final[-1][3][1:-1].split(\", \") and mut_and_snip[0] not in final[-2][3][1:-1].split(\", \")) \\\n",
    "                            and mut_and_snip[0] not in mutations:\n",
    "                    mutations.append(mut_and_snip[0])\n",
    "            if mutations:\n",
    "                mutations = \"'\" + \"', '\".join(mutations) + \"'\"\n",
    "                print(1, mutations)\n",
    "                final.append([paper_id, 'Regex', var_plus_genes, mutations, 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "            break\n",
    "\n",
    "        output = ner_score(sentence.strip())\n",
    "        if output:\n",
    "            mutations = []\n",
    "            for mut_and_snip in output:\n",
    "                # temp fix to deal with same mutation getting detected due to stiching multiple sentences\n",
    "                if (mut_and_snip[0] not in final[-1][3][1:-1].split(\", \") and mut_and_snip[0] not in final[-2][3][1:-1].split(\", \")) \\\n",
    "                        and not all(len(word) < 4 for word in mut_and_snip[0].split())\\\n",
    "                    and mut_and_snip[0] not in mutations:\n",
    "                    mutations.append(mut_and_snip[0])\n",
    "            if mutations:\n",
    "                mutations = \"'\" + \"', '\".join(mutations) + \"'\"\n",
    "                print(2, mutations)\n",
    "                final.append([paper_id, 'NER', var_plus_genes, mutations, 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "            break\n",
    "        \n",
    "        # these data, if found, are going to be important if no mutations are in that sentence\n",
    "        if var_plus_genes:\n",
    "            final.append([paper_id, '', var_plus_genes, '', 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "        \n",
    "        not_single_sentence = True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total sentences to process  492\n",
      "{total sentences processed}>{sentences with mutation}\n",
      "2 'mutations in conserved regions'\n",
      "1 'C777Y'\n",
      "1 'R739H'\n",
      "1 'R741H', 'R749P'\n",
      "1 'G785E'\n",
      "2 'alteration of the corre'\n",
      "50>11 1 '1051 N/I', '1086 G/E', '1110 G/E', '1336 S/F', '1406 G/R', '363 A/V', '636 G/R', '739 R/H', '746 A /V', '777 C/Y', '785 G/E', '869 V/M', 'A 764 C', 'C  726 R', 'C  772 G', 'C 1038 T', 'C 1073 G', 'C 1318 T', 'C 1388 G', 'C 355 G', 'C 623 G', 'C 856 T', 'C/Y 1051', 'C/Y 1086', 'C/Y 1110', 'C/Y 1336', 'C/Y 1406', 'C/Y 363', 'C/Y 636', 'C/Y 869', 'E  777 C', 'G  764 C', 'G/E 777', 'G/R 746', 'G/R 777', 'N/I 777', 'V 777 C', 'V/M 777', 'Y 1051 N', 'Y 1086 G', 'Y 1336 S', 'Y 1406 G', 'Y 363 A', 'Y 636 G', 'Y 869 V'\n",
      "1 'C777Y', 'G785E'\n",
      "1 'G636R'\n",
      "1 'A746V'\n",
      "1 'S678N'\n",
      "2 'mutations in the funnel domain'\n",
      "1 'S747L'\n",
      "2 'amino acid corresponding to m322'\n",
      "1 'V869M'\n",
      "1 'G1086E', 'G1110E'\n",
      "2 'substitutions at either glycine'\n",
      "1 'G1110 to E'\n",
      "100>27 1 'G1406R'\n",
      "1 'S1336F'\n",
      "1 'A363V'\n",
      "2 'a363 is also predicted'\n",
      "1 'N1051I'\n",
      "2 'yeast amino acid position corresponding to n1051 [ t1038'\n",
      "2 'n to i substitution'\n",
      "2 'prox - imity to bridge helix ( bottom a -'\n",
      "150>38 200>38 250>43 2 'missense mutants were found in a 16 amino acid stretch of the 627 amino'\n",
      "2 'in frame deletion of this region'\n",
      "1 'E162K'\n",
      "2 'stop codon and a deletion mutant'\n",
      "2 'in - frame deletion of the 15 amino acids shown'\n",
      "2 'two stop codons in frame ( indicated by x )'\n",
      "2 'insertion of four amino acids next to residue 168'\n",
      "2 'point mutation in the'\n",
      "2 'truncated after the sds domain'\n",
      "300>53 2 'gfp inserted into the coding region'\n",
      "350>57 2 'c subelement removed from its normal context'\n",
      "400>61 450>61 "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "temp = final[2:] # removing the temporary first row and header\n",
    "np.save('data/model_output/results.npy', temp)\n",
    "\n",
    "# columns with asterisk contain data which is useful regardless of whether the sentence \n",
    "# contains mutation info\n",
    "data = pd.DataFrame(temp[:], columns=['WBPaper ID', 'Method', '* Gene-Variant combo ', 'Mutation', 'Sentence'])\n",
    "data.to_csv(\"data/model_output/extracted_snippets.csv\", index=False, encoding='utf-8')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking which papers had zero mutations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = np.load('data/model_output/results.npy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in np.unique(ids_to_extract):\n",
    "    if i not in results[:, 0]:\n",
    "        print(i, end= ' ')\n",
    "        continue"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90bac3f7a4bb879b9d06605bdeda624e0779c88b1a5b8631d7aaa6d430fa2aec"
  },
  "kernelspec": {
   "display_name": "wb_env",
   "language": "python",
   "name": "wb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}