{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For training BioBERT NER on nala\r\n",
    "Run this in CLI -  \r\n",
    "\r\n",
    "python train_ner.py --model_name_or_path dmis-lab/biobert-base-cased-v1.1 --train_file data/nala/train_dev.json --validation_file data/nala/devel.json --text_column_name tokens --label_column_name tags --pad_to_max_length --max_length 192 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 10 --output_dir models/nala --seed 1\r\n",
    "  \r\n",
    "Note:  Training took <30 mins on 1660 ti. You can decrease the num_train_epochs count to 5 without any substantial difference in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Regex \r\n",
    "## Has high precision and low recall.    \r\n",
    "## Consists of 3 parts - MutationFinder, tmVar and some custom patterns from WB papers \r\n",
    "### 1.1 Mutation Finder [Link](https://github.com/divyashan/MutationFinder), Modified regex from SETH [Link](https://github.com/rockt/SETH/blob/master/resources/mutations.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc.regex_block import mutation_finder_from_regex_filepath, TmVar, BOWdictionary, CustomWBregex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_regex_path = 'data/regexs/mutationfinder_regex/seth_modified.txt'\r\n",
    "mf_mut_extract = mutation_finder_from_regex_filepath(mf_regex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(1154)C : A(1154)C\n"
     ]
    }
   ],
   "source": [
    "text = 'A(1154)C'\r\n",
    "for mutation, snip in mf_mut_extract(raw_text=text, span_size=150).items():\r\n",
    "    mutation_entry = snip + ' : ' + mutation.OriginalMention\r\n",
    "    print(mutation_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 tmVar [Link](https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/tmvar/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Rishab/Documents/GitHub/genomic-info-from-papers/genomic-info-from-papers/utils/misc/regex_block.py:302: FutureWarning: Possible nested set at position 15\n",
      "  self._regular_expressions.append(re.compile(reg))\n"
     ]
    }
   ],
   "source": [
    "tmvar_mut_extract = TmVar('data/regexs/tmvar_regex/final_regex_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A347V', ' n2923 (A347V), n2870(R429K), and n1163(S486F'],\n",
       " ['R429K', ' n2923 (A347V), n2870(R429K), and n1163(S486F']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmvar_mut_extract(' n2923 (A347V), n2870(R429K), and n1163(S486F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extra custom regexs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utils/all_config.cfg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_config = configparser.ConfigParser()\r\n",
    "db_config.read('utils/all_config.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mut_extract = CustomWBregex(db_config, extra_regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the statement ced-3(n2888) \r\n",
    "n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT',\n",
       "  'n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_mut_extract('n2888 bp AAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ced-3', 'Just gene']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_mut_extract.get_genes('ced-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_mut_extract = BOWdictionary()\r\n",
    "# bow_mut_extract('This mutation deletes 471bp of the promoter region, the transcriptional start and 56 amino acids of the second exon.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 MF + tmVar + Custom regex + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_rows(a):\n",
    "    a = np.ascontiguousarray(a)\n",
    "    unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "    return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))\n",
    "\n",
    "\n",
    "def regex_block(sentence, span_size=150):\n",
    "    mut_and_snippets = []\n",
    "    \n",
    "    # MutationFinder\n",
    "    for mutation, snip in mf_mut_extract(raw_text=sentence, span_size=span_size).items():\n",
    "        mut_and_snippets.append([mutation.OriginalMention, snip])\n",
    "    \n",
    "    # tmVar\n",
    "    mut_and_snippets = mut_and_snippets + tmvar_mut_extract(sentence, span_size=span_size)\n",
    "    # Custom patterns\n",
    "    mut_and_snippets = mut_and_snippets + custom_mut_extract(sentence, span_size=span_size)\n",
    "    # Bag of words\n",
    "    mut_and_snippets = mut_and_snippets + bow_mut_extract(sentence)\n",
    "    \n",
    "    if mut_and_snippets:\n",
    "        mut_and_snippets = unique_rows(mut_and_snippets).tolist()\n",
    "    return mut_and_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_block(' asdf gpa-2 ::Tc1 asdf as')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 * Additional details  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_info_block(sentence, span_size=150):\r\n",
    "    info_and_snippets = []\r\n",
    "\r\n",
    "    # look for gene and variant combo\r\n",
    "    info_and_snippets = info_and_snippets + custom_mut_extract.var_and_gene_close(sentence, span_size=span_size)\r\n",
    "    \r\n",
    "    if info_and_snippets:\r\n",
    "        info_and_snippets = unique_rows(info_and_snippets).tolist()\r\n",
    "    return info_and_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ced-3(n2888', 'Gene & Variant']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_info_block('in the statement ced-3(n2888)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in scores\r\n",
    "MF + tmVar only: on remarks - (0.9459459459459459, 0.39923954372623577, 0.5614973262032086, None)   \r\n",
    "on remarks + nala - (0.8671477079796265, 0.6698360655737705, 0.755826859045505, None)  \r\n",
    "    \r\n",
    "MF + tmVar + Custom + BOW: on remarks - (0.9606741573033708, 0.6501901140684411, 0.7755102040816326, None)  \r\n",
    "on remarks + nala - (0.8680868496517821, 0.6947540983606557, 0.7718084137679839, None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 BioBERT NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/risubu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\r\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\r\n",
    "from transformers import TokenClassificationPipeline\r\n",
    "\r\n",
    "import re\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import sklearn as sk\r\n",
    "import math \r\n",
    "import string\r\n",
    "import time\r\n",
    "import json\r\n",
    "import csv\r\n",
    "import shutil\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'models/nala'\r\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n",
    "model = AutoModelForTokenClassification.from_pretrained(\r\n",
    "    model_name_or_path,\r\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\r\n",
    "    config=config,\r\n",
    ")\r\n",
    "# LABEL_0 - B-mut, LABEL_1 - I-mut, LABEL_2 - O\r\n",
    "nala_ner  = TokenClassificationPipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\r\n",
    "stop_words = [w for w in stop_words if len(w) > 1]\r\n",
    "\r\n",
    "def ner_score(sentence):\r\n",
    "    mutations = []\r\n",
    "    try:\r\n",
    "        ner_output = nala_ner(sentence)\r\n",
    "        for i, grp in enumerate(ner_output):\r\n",
    "            if grp['entity_group'] == 'LABEL_0':\r\n",
    "                mut = grp['word']\r\n",
    "                for j in range(i+1, len(ner_output)):\r\n",
    "                    if ner_output[j]['entity_group'] == 'LABEL_1':\r\n",
    "                        mut  = mut + ' ' + ner_output[j]['word']\r\n",
    "                    else:\r\n",
    "                        # NER would be handling only data in NL form\r\n",
    "                        if len(mut.split()) > 3 and any(word in mut.split() for word in stop_words):\r\n",
    "                            mutations.append([mut, sentence])\r\n",
    "                        break\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    return mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Testing \r\n",
    "## 3.2 On scored curator remarks and nala mutation corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_texts = []\r\n",
    "# issues_count = 0\r\n",
    "\r\n",
    "# df = pd.read_csv(r\"data\\gsoc\\Remarks_scored.csv\")\r\n",
    "# df = df.to_numpy()\r\n",
    "# # extract sentences\r\n",
    "# text = []\r\n",
    "# y_true = []\r\n",
    "# for idx, row in enumerate(df):\r\n",
    "#     loc = str(row[2]).find('Paper_evidence')\r\n",
    "#     if loc != -1:\r\n",
    "#         if row[0].split()[0] == 'Yes':\r\n",
    "#             y_true.append(1)\r\n",
    "#         elif row[0].split()[0] == 'No':\r\n",
    "#             y_true.append(0)\r\n",
    "#         else:\r\n",
    "#             continue\r\n",
    "#         temp_str = str(row[2][1:loc-2]).replace(\"\\\"\", \"'\")\r\n",
    "#         text.append(temp_str)\r\n",
    "\r\n",
    "# assert len(text) == len(y_true)\r\n",
    "# print('Count from Remarks_scored = {}'.format(len(text)))\r\n",
    "\r\n",
    "# y_pred = []\r\n",
    "# for sentence in text:\r\n",
    "#     if regex_block(sentence):\r\n",
    "#         y_pred.append(1)\r\n",
    "#     elif ner_score(sentence):\r\n",
    "#         y_pred.append(1)\r\n",
    "#     else:\r\n",
    "#         y_pred.append(0)\r\n",
    "\r\n",
    "# assert len(y_pred) == len(y_true)\r\n",
    "# all_texts = all_texts + text\r\n",
    "\r\n",
    "\r\n",
    "# df = pd.read_csv(r\"data\\nala\\binary_nala_NOT_NER.csv\")\r\n",
    "# df = df.to_numpy()\r\n",
    "# print('Count from nala = {}'.format(len(df)))\r\n",
    "\r\n",
    "# print('Entries processed: ', end=' ')\r\n",
    "# for i, row in enumerate(df):\r\n",
    "#     if i%500 == 0: print(i, end=' ')\r\n",
    "#     sentence = row[0]\r\n",
    "#     label = row[1]\r\n",
    "#     try:\r\n",
    "#         if regex_block(sentence):\r\n",
    "#             y_pred.append(1)\r\n",
    "#         elif ner_score(sentence):\r\n",
    "#             y_pred.append(1)\r\n",
    "#         else:\r\n",
    "#             y_pred.append(0)\r\n",
    "\r\n",
    "#         if label == 1:\r\n",
    "#             y_true.append(1)\r\n",
    "#         else:\r\n",
    "#             y_true.append(0)\r\n",
    "        \r\n",
    "#         all_texts.append(sentence)\r\n",
    "#     except:\r\n",
    "#         issues_count += 1\r\n",
    "#         pass\r\n",
    "\r\n",
    "# assert len(y_pred) == len(y_true) == len(all_texts)\r\n",
    "# print('\\nTotal count = {}'.format(len(y_pred)))\r\n",
    "# if issues_count: print('Note: Could not process {} sentences'.format(issues_count))\r\n",
    "\r\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually inspecting incorrect preds\r\n",
    "# bad = []\r\n",
    "# assert len(y_pred) == len(y_true) == len(all_texts)\r\n",
    "# for i, (t, p,sent) in enumerate(zip(y_true, y_pred, all_texts)):\r\n",
    "#     if t != p:\r\n",
    "#         if t:\r\n",
    "#             bad.append([sent,i])\r\n",
    "# len(bad)\r\n",
    "# # print(bad[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in scores (find the test cell block below)\r\n",
    "  \r\n",
    "Regex only: on remarks - (0.9606741573033708, 0.6501901140684411, 0.7755102040816326, None)  \r\n",
    "on remarks + nala - (0.8680868496517821, 0.6947540983606557, 0.7718084137679839, None) \r\n",
    "  \r\n",
    "Regex + NER: on remarks - (0.8592057761732852, 0.9049429657794676, 0.8814814814814815, None)   \r\n",
    "on remarks + nala - (0.7570694087403599, 0.9655737704918033, 0.8487031700288185, None)  \r\n",
    "  \r\n",
    "### Retraining on NL and SST  \r\n",
    "Regex + NER: on remarks - (0.9627906976744186, 0.7870722433460076, 0.8661087866108786, None)      \r\n",
    "on remarks + nala - (0.8461816865725661, 0.7665573770491804, 0.804403922243248, None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 On WB papers\r\n",
    "### Get the paper texts from textpresso API and wbtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works - First paper ID is searched through textpresso API. If the recieved output is blank, then wbtools is used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import argparse\n",
    "import subprocess\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import nltk.data\n",
    "import os\n",
    "from xml.dom import minidom\n",
    "from bs4 import BeautifulSoup\n",
    "from wbtools.literature.corpus import CorpusManager\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the cell below only once to save the paper sentences in your local computer to avoid pulling paper text everytime during dev  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreate a separate temporary numpy file to store the paper sentences  \\nto avoid spending time on pulling sentences while in development \\nFormat - [paper_id, sentence]\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create a separate temporary numpy file to store the paper sentences  \n",
    "to avoid spending time on pulling sentences while in development \n",
    "Format - [paper_id, sentence]\n",
    "'''\n",
    "# final_data = []\n",
    "# # remove_sections = [PaperSections.ACKNOWLEDGEMENTS, PaperSections.REFERENCES, PaperSections.RELATED_WORK, PaperSections.INTRODUCTION]\n",
    "# remove_sections = []\n",
    "# # random 100 papers mentioned the remarks ace file in data/gsoc\n",
    "# paper_ids = np.load('data/top100.npy').tolist()\n",
    "# cm = CorpusManager()\n",
    "# for i, paper_id in enumerate(paper_ids):\n",
    "#     paper_id = paper_id[7:]\n",
    "#     cm.load_from_wb_database(db_name=db_config['wb_database']['db_name'], db_user=db_config['wb_database']['db_user'], db_password=db_config['wb_database']['db_password'],\n",
    "#         db_host=db_config['wb_database']['db_host'], paper_ids=[paper_id],\n",
    "#         ssh_host=db_config['wb_database']['ssh_host'], ssh_user=db_config['wb_database']['ssh_user'], ssh_passwd=db_config['wb_database']['ssh_passwd'],\n",
    "#         load_bib_info=False, load_afp_info=False, load_curation_info=False)\n",
    "#     sentences = cm.get_paper(paper_id).get_text_docs(remove_sections=remove_sections,split_sentences=True)\n",
    "#     for sent in sentences:\n",
    "#         final_data.append([paper_id, sent])\n",
    "#     print(i, end = \" \")\n",
    "# final_data = pd.DataFrame(final_data[:], columns=['WBPaper ID', 'Sentence'])\n",
    "# final_data.to_csv(\"data/id_and_sentence.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textpresso_paper_text(wbpid, path, token):\n",
    "    \"\"\"This sub takes a wbpid eg WBPaper00056731 and returns the fulltext paper in sentences\"\"\"\n",
    "    ft=[0];\n",
    "    # Check that wbpid is a valid WBPaper\n",
    "    if not re.match( 'WBPaper', wbpid):\n",
    "        print (wbpid, \"is not a valid WBPaper ID\")\n",
    "        return ft\n",
    "    # Download paper if it doesn't exist\n",
    "    fn = path + '/temp/' + wbpid + '.json'\n",
    "\n",
    "    if os.path.exists(fn) and os.path.getsize(fn) > 16:\n",
    "        pass\n",
    "    else:\n",
    "        com1 = '-o '+fn +'\\n-k '+ '\\n'+'-d \"{\\\\\"token\\\\\":\\\\\"'+ token + '\\\\\", \\\\\"query\\\\\": {\\\\\"accession\\\\\": \\\\\"' + wbpid +'\\\\\", \\\\\"type\\\\\": \\\\\"document\\\\\", \\\\\"corpora\\\\\": [\\\\\"C. elegans\\\\\"]}, \\\\\"include_fulltext\\\\\": true}\"'\n",
    "        configf= path +'/temp/' + wbpid + '.tmp.config'\n",
    "        curlf = open(configf,'w')\n",
    "        print (com1, file=curlf)\n",
    "        curlf.close()\n",
    "        command = 'curl -o '+ fn +' -K '+ configf+' https://textpressocentral.org:18080/v1/textpresso/api/search_documents' \n",
    "        comlist = command.split()\n",
    "        os.system(command)\n",
    "\n",
    "    # Read the paper, and split into sentences\n",
    "    if os.path.exists(fn) and os.path.getsize(fn) > 20:\n",
    "        # Open our JSON file and load it into python\n",
    "        input_file = open (fn)\n",
    "        json_array = json.load(input_file)\n",
    "        for item in json_array:\n",
    "            abs = item[\"abstract\"]\n",
    "            fullt =  item[\"fulltext\"]\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            ft = tokenizer.tokenize(abs)\n",
    "            ftt=tokenizer.tokenize(fullt)\n",
    "            ft = ft +ftt\n",
    "    else:\n",
    "        # some paper texts are blank for some reason\n",
    "        # pipeline uses wbtools to get sentences in such case\n",
    "        pass\n",
    "\n",
    "    outfilen = os.path.join(path, 'text_flatfiles', wbpid+'.txt')\n",
    "    outf = open(outfilen, 'w')\n",
    "    for sen in ft:\n",
    "        sen =str(sen)\n",
    "        print(sen, file=outf)\n",
    "    outf.close()\n",
    "    return outfilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentences out of wbtools are sometimes weird, especially for the old papers\n",
    "This increases the false neg rate so this is used only when the textpresso api provides bad text\n",
    "Most of the issues have band aid fixes in the text preprocessing cell below but there are some \n",
    "with no easy fix or need to be worked on - \n",
    "1. Table content is extracted column wise, not row wise \n",
    "2. Some sentences have white space between every character and are also somehow inverted (???)\n",
    "    e.g. Check out the sentences of WBPaper00002018 (1994)\n",
    "    Line 154 -  '0 7 1 u ( 7 - c e m    ) F (   .'\n",
    "    inverted and without the white space is (mec-7)u170 which is extremely useful and will get missed \n",
    "    by the pipeline unless processed correctly.\n",
    "TODO: Not sure how to solve point 1 but point 2 is easy to solve and also helps a LOT.\n",
    "    Rishab, work on this after you complete the project. \n",
    "    Not high priority as this might be only for the >10 year old papers (which are already manually curated)\n",
    "'''\n",
    "cm = CorpusManager()\n",
    "def wbtools_paper_text(wbpid, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "    ssh_user, ssh_passwd):\n",
    "    # sectioning might not be always correct, text processing is done separately in the pipeline\n",
    "    # remove_sections = [PaperSections.ACKNOWLEDGEMENTS, PaperSections.REFERENCES, PaperSections.RELATED_WORK, PaperSections.INTRODUCTION]\n",
    "    remove_sections = []\n",
    "    paper_id = wbpid[7:]\n",
    "    cm.load_from_wb_database(db_name=db_name, db_user=db_user, db_password=db_password,\n",
    "        db_host=db_host, paper_ids=[paper_id],\n",
    "        ssh_host=ssh_host, ssh_user=ssh_user, ssh_passwd=ssh_passwd,\n",
    "        load_bib_info=False, load_afp_info=False, load_curation_info=False)\n",
    "    sentences = cm.get_paper(paper_id).get_text_docs(remove_sections=remove_sections,split_sentences=True)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_sentences(wbpids, config, store_ppr_path):\n",
    "    '''\n",
    "    Takes WB Paper IDs and returns a list of sentences from those papers after filtering\n",
    "    Arg:\n",
    "    wbpids - List of wb papers ids \n",
    "        e.g. ['WBPaper00002379']\n",
    "    config_path - Config file path\n",
    "    store_ppr_path - Folder path to store the paper flatfiles retrieved from TextPresso for future use\n",
    "    Returns:\n",
    "    paperid_sentence_list: List of paper ID and sentence\n",
    "        e.g. [['WBPaper00002379', 'First sentence'], ['WBPaper00002379', 'Second sentence'], ....]\n",
    "    '''\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words = [w for w in stop_words if len(w) > 1]\n",
    "\n",
    "    all_special_chars = []\n",
    "    with open('data/nala/train_dev.json') as f:\n",
    "        for jsonObj in f:\n",
    "            nala_json = json.loads(jsonObj)['tokens']\n",
    "            for word in nala_json:\n",
    "                if not word.isalnum():\n",
    "                    all_special_chars.append(word)\n",
    "    # list of special characters to keep during inference\n",
    "    # helps with clearing out the bad characters from old papers\n",
    "    all_special_chars = list(set(all_special_chars))\n",
    "    \n",
    "    token = config['textpresso']['token']\n",
    "    db_name=config['wb_database']['db_name']\n",
    "    db_user=config['wb_database']['db_user']\n",
    "    db_password=config['wb_database']['db_password']\n",
    "    db_host=config['wb_database']['db_host']\n",
    "    ssh_host=config['wb_database']['ssh_host']\n",
    "    ssh_user=config['wb_database']['ssh_user']\n",
    "    ssh_passwd=config['wb_database']['ssh_passwd']\n",
    "\n",
    "    temp_paperid_sentence = np.array([])\n",
    "    if os.path.isfile('data/id_and_sentence.csv'):\n",
    "        temp_paperid_sentence = pd.read_csv(\"data/id_and_sentence.csv\", lineterminator='\\n', dtype = str).to_numpy() # WBPaper ID, Sentence\n",
    "    paperid_sentence_list = []\n",
    "    \n",
    "    for curr_ppr_i, id in enumerate(wbpids):\n",
    "        print(f\"{curr_ppr_i+1}\", end = \" \")\n",
    "        # textpresso_paper_text() also saves the text in flatfiles for future use \n",
    "        start_time = time.time()\n",
    "        paper_path = textpresso_paper_text(id, store_ppr_path, token)\n",
    "        txt = Path(paper_path).read_text().split('\\n')\n",
    "        # deals with empty text files with only \"0\"\n",
    "        if len(txt) == 2:\n",
    "            if temp_paperid_sentence.size != 0:\n",
    "                txt = temp_paperid_sentence[temp_paperid_sentence[:, 0] == id[7:]][:, 1]\n",
    "                # incase the loaded numpy file didn't have the required paper \n",
    "                if len(txt) == 0 and platform.system() != 'Windows':\n",
    "                    txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                        ssh_user, ssh_passwd)\n",
    "            elif platform.system() != 'Windows' :\n",
    "                txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                    ssh_user, ssh_passwd)\n",
    "            \n",
    "        for row in txt: \n",
    "            if row.find('fifi') != -1:\n",
    "                if temp_paperid_sentence.size != 0:\n",
    "                    txt = temp_paperid_sentence[temp_paperid_sentence[:, 0] == id[7:]][:, 1]\n",
    "                    # incase the loaded numpy file didn't have the required paper \n",
    "                    if len(txt) == 0 and platform.system() != 'Windows':\n",
    "                        txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                            ssh_user, ssh_passwd)\n",
    "                elif platform.system() != 'Windows':\n",
    "                    txt = wbtools_paper_text(id, db_name, db_user, db_password, db_host, ssh_host,\\\n",
    "                        ssh_user, ssh_passwd)\n",
    "                break\n",
    "            \n",
    "        count_total_rows = len(txt)\n",
    "        for current_i, row in enumerate(txt):\n",
    "            if row.lower().find(\"we thank\") == 0 or row.lower().find(\"this work was supported\") == 0 \\\n",
    "                or row.lower().find(\"references\") == 0 or row.lower().find(\"we also thank\") == 0 \\\n",
    "                or row.lower().find(\"this research was supported\") == 0 or row.lower().find(\"we acknowledge\") == 0 \\\n",
    "                or row.lower().find(\"acknowledgments\") == 0 or row.lower().find('literature cited') != -1:\n",
    "                if current_i > count_total_rows/3:\n",
    "                    break\n",
    "\n",
    "            # usually is bad sentence\n",
    "            if len(row) < 40 or not any(word in row.lower().split() for word in stop_words):\n",
    "                continue\n",
    "            # remove sentences with links and email ids\n",
    "            if re.search('\\S+@\\S+\\.', row) or re.search('www\\.\\S+\\.', row):\n",
    "                continue\n",
    "            # filters one word sentences\n",
    "            if len(row.split()) == 1:\n",
    "                continue\n",
    "            # sentences comprised of only single characters \n",
    "            # ^ seems to be issue with wbtools extraction pipeline \n",
    "            if all(len(word) < 5 for word in row.split()):\n",
    "                continue\n",
    "            row = re.sub(\"\\( *cid *: *\\d+ *\\)\", \" \", row)\n",
    "            # TODO: replace this block with a regex sub \n",
    "            temp_row = row\n",
    "            for c in temp_row:\n",
    "                if (not c.isalnum() and not c == ' ') and c not in all_special_chars:\n",
    "                        row = row.replace(c, \"\")\n",
    "                        \n",
    "#             fixes bad space between each character of flanking sequence from old papers\n",
    "#             Switching this off as it increases the processing time\n",
    "#             also affects very small subset of old papers so not worth the extra time\n",
    "            flanking_regex = re.compile('([ACTG]( +)){4,}')\n",
    "            for m in flanking_regex.finditer(row):\n",
    "                span = (m.start(0), m.end(0))   \n",
    "                span = row[span[0]:span[1]-1]\n",
    "                correct_flank = re.sub('([ACTG])( +)', r'\\1', row)\n",
    "                row = row.replace(span, correct_flank)\n",
    "            paperid_sentence_list.append((id, row))\n",
    "    return paperid_sentence_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   152  100     4  100   148      1     72  0:00:04  0:00:02  0:00:02    74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 "
     ]
    }
   ],
   "source": [
    "# papers mentioned the remarks ace file in data/gsoc\n",
    "ids_to_extract = np.load('data/top100.npy').tolist()[:-20]\n",
    "paperid_sentence_list = get_paper_sentences(ids_to_extract, db_config, store_ppr_path='data/wbpapers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates keeping the order\n",
    "seen = set()\n",
    "paperid_sentence_list = np.array([x for x in paperid_sentence_list if x not in seen and not seen.add(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences and characters:  25476 4097379\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences and characters: ', end=' ')\n",
    "print(len(paperid_sentence_list), sum([len(sent[1]) for sent in paperid_sentence_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell takes a while to run - mainly due to the huge regex blocks   \n",
    "~1 hour per 10 papers   \n",
    "Can't really switch them off though. There might be a smarter way to work with the regex block but this pipeline would be probably running every month on 50 or so papers so low priority.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences to process  25476\n",
      "{total sentences processed}>{snippets with useful info}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50>8 100>13 150>17 200>21 250>28 300>40 350>55 400>56 450>69 500>94 550>101 600>112 650>146 700>160 750>196 800>244 850>290 900>335 950>367 1000>371 1050>380 1100>386 1150>395 1200>414 1250>416 1300>416 1350>433 1400>454 1450>459 1500>471 1550>477 1600>484 1650>486 1700>497 1750>505 1800>523 1850>524 1900>544 1950>570 2000>599 2050>643 2100>685 2150>737 2200>779 2250>824 2300>842 2350>854 2400>880 2450>905 2500>945 2550>979 2600>993 2650>1017 2700>1037 2750>1069 2800>1099 2850>1148 2900>1192 2950>1228 3000>1250 3050>1266 3100>1295 3150>1296 3200>1305 3250>1325 3300>1329 3350>1370 3400>1412 3450>1449 3500>1459 3550>1478 3600>1499 3650>1502 3700>1502 3750>1504 3800>1505 3850>1506 3900>1509 3950>1523 4000>1534 4050>1550 4100>1570 4150>1594 4200>1614 4250>1634 4300>1653 4350>1684 4400>1724 4450>1767 4500>1801 4550>1834 4600>1860 4650>1870 4700>1919 4750>1970 4800>2016 4850>2043 4900>2066 4950>2088 5000>2109 5050>2155 5100>2208 5150>2253 5200>2292 5250>2306 5300>2318 5350>2351 5400>2382 5450>2415 5500>2457 5550>2497 5600>2518 5650>2559 5700>2584 5750>2605 5800>2639 5850>2682 5900>2730 5950>2779 6000>2819 6050>2855 6100>2896 6150>2928 6200>2965 6250>2990 6300>2991 6350>2991 6400>3013 6450>3045 6500>3065 6550>3066 6600>3068 6650>3080 6700>3100 6750>3141 6800>3185 6850>3237 6900>3282 6950>3339 7000>3379 7050>3424 7100>3469 7150>3514 7200>3563 7250>3597 7300>3640 7350>3694 7400>3736 7450>3762 7500>3780 7550>3818 7600>3838 7650>3883 7700>3927 7750>3973 7800>4013 7850>4050 7900>4082 7950>4130 8000>4174 8050>4213 8100>4246 8150>4300 8200>4351 8250>4396 8300>4413 8350>4419 8400>4448 8450>4474 8500>4495 8550>4504 8600>4514 8650>4546 8700>4578 8750>4599 8800>4621 8850>4643 8900>4666 8950>4701 9000>4734 9050>4760 9100>4797 9150>4829 9200>4855 9250>4883 9300>4908 9350>4949 9400>4981 9450>5011 9500>5038 9550>5054 9600>5084 9650>5116 9700>5151 9750>5176 9800>5211 9850>5260 9900>5313 9950>5340 10000>5366 10050>5404 10100>5445 10150>5477 10200>5509 10250>5522 10300>5538 10350>5545 10400>5559 10450>5576 10500>5591 10550>5592 10600>5619 10650>5641 10700>5671 10750>5696 10800>5723 10850>5758 10900>5793 10950>5823 11000>5856 11050>5897 11100>5941 11150>5965 11200>6017 11250>6074 11300>6115 11350>6158 11400>6189 11450>6232 11500>6270 11550>6318 11600>6365 11650>6395 11700>6400 11750>6449 11800>6499 11850>6529 11900>6561 11950>6578 12000>6609 12050>6610 12100>6620 12150>6629 12200>6649 12250>6672 12300>6706 12350>6728 12400>6735 12450>6741 12500>6765 12550>6801 12600>6836 12650>6864 12700>6892 12750>6921 12800>6978 12850>7034 12900>7090 12950>7100 13000>7132 13050>7143 13100>7154 13150>7182 13200>7224 13250>7261 13300>7322 13350>7364 13400>7412 13450>7461 13500>7511 13550>7545 13600>7575 13650>7599 13700>7624 13750>7669 13800>7733 13850>7797 13900>7850 13950>7900 14000>7915 14050>7929 14100>7939 14150>7964 14200>7993 14250>8015 14300>8038 14350>8047 14400>8067 14450>8081 14500>8090 14550>8093 14600>8113 14650>8128 14700>8151 14750>8179 14800>8214 14850>8266 14900>8315 14950>8357 15000>8398 15050>8419 15100>8459 15150>8514 15200>8578 15250>8633 15300>8677 15350>8707 15400>8734 15450>8753 15500>8786 15550>8824 15600>8876 15650>8896 15700>8923 15750>8949 15800>8987 15850>9025 15900>9045 15950>9062 16000>9078 16050>9106 16100>9127 16150>9141 16200>9153 16250>9173 16300>9201 16350>9227 16400>9269 16450>9317 16500>9365 16550>9421 16600>9452 16650>9481 16700>9492 16750>9533 16800>9569 16850>9613 16900>9645 16950>9681 17000>9715 17050>9752 17100>9779 17150>9802 17200>9834 17250>9870 17300>9911 17350>9941 17400>9984 17450>9992 17500>10035 17550>10079 17600>10114 17650>10145 17700>10184 17750>10224 17800>10273 17850>10309 17900>10347 17950>10386 18000>10428 18050>10451 18100>10477 18150>10500 18200>10529 18250>10568 18300>10602 18350>10621 18400>10648 18450>10679 18500>10726 18550>10787 18600>10816 18650>10841 18700>10860 18750>10903 18800>10939 18850>10973 18900>11016 18950>11044 19000>11070 19050>11102 19100>11150 19150>11180 19200>11215 19250>11272 19300>11318 19350>11368 19400>11416 19450>11439 19500>11468 19550>11514 19600>11557 19650>11594 19700>11633 19750>11673 19800>11712 19850>11757 19900>11799 19950>11843 20000>11881 20050>11925 20100>11965 20150>12011 20200>12065 20250>12100 20300>12134 20350>12165 20400>12200 20450>12234 20500>12267 20550>12300 20600>12347 20650>12385 20700>12435 20750>12480 20800>12534 20850>12591 20900>12644 20950>12676 21000>12712 21050>12751 21100>12781 21150>12815 21200>12858 21250>12879 21300>12920 21350>12951 21400>12973 21450>13005 21500>13026 21550>13056 21600>13083 21650>13104 21700>13139 21750>13164 21800>13188 21850>13219 21900>13254 21950>13285 22000>13313 22050>13328 22100>13344 22150>13384 22200>13428 22250>13477 22300>13514 22350>13529 22400>13572 22450>13628 22500>13682 22550>13734 22600>13768 22650>13791 22700>13814 22750>13849 22800>13883 22850>13909 22900>13930 22950>13954 23000>13986 23050>14027 23100>14075 23150>14117 23200>14156 23250>14185 23300>14238 23350>14306 23400>14369 23450>14423 23500>14467 23550>14502 23600>14520 23650>14557 23700>14575 23750>14608 23800>14634 23850>14678 23900>14695 23950>14715 24000>14753 24050>14792 24100>14826 24150>14842 24200>14868 24250>14890 24300>14917 24350>14952 24400>14997 24450>15023 24500>15048 24550>15080 24600>15109 24650>15151 24700>15186 24750>15230 24800>15282 24850>15338 24900>15377 24950>15402 25000>15404 25050>15404 25100>15404 25150>15407 25200>15408 25250>15431 25300>15459 25350>15509 25400>15556 25450>15611 Total time for processing in minutes:  612\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "final = [\n",
    "    ['temporary', 'temporary', 'temporary', 'temporary', 'temporary', 'temporary'],\\\n",
    "    ['WBPaper ID', 'Method', '*Genes', '*Gene-Variant combo', 'Mutation', 'Sentence']]\n",
    "total_sentences = len(paperid_sentence_list)\n",
    "print_snips = False\n",
    "print('Total sentences to process ', len(paperid_sentence_list))\n",
    "# there would be lots of duplicates in the gene col which might cause high count of\n",
    "# snippets with useful info. they'll get filtered out in the third notebook\n",
    "print('{total sentences processed}>{snippets with useful info}')\n",
    "for ppr_sen_idx, row in enumerate(paperid_sentence_list):\n",
    "    if (ppr_sen_idx+1) % 50 == 0: print(f\"{ppr_sen_idx+1}>{len(final)-1}\", end = \" \")\n",
    "    paper_id = row[0]\n",
    "    sentence = str()\n",
    "    limit = min(ppr_sen_idx+2, total_sentences)\n",
    "    # some sentences - mostly table content are super long\n",
    "    # temp fix, need to have a nice sentence splitter to minimize manual verification time\n",
    "    not_single_sentence = False\n",
    "    for i in range(ppr_sen_idx, limit):\n",
    "\n",
    "        sentence = sentence + paperid_sentence_list[i][1] + ' '\n",
    "\n",
    "        if (len(sentence) > 250 and not_single_sentence):\n",
    "            break\n",
    "        if paper_id != paperid_sentence_list[i][0]:\n",
    "            break\n",
    "        \n",
    "        var_plus_genes = ''\n",
    "        # Look for the special data e.g. gene-variant combo (e.g 'ced-3(n2888)') only on single sentences \n",
    "        if not not_single_sentence:\n",
    "            var_plus_genes = []\n",
    "            all_genes = []\n",
    "            \n",
    "            for data_and_cat in custom_mut_extract.var_and_gene_close(sentence.strip()):\n",
    "                var_plus_genes.append(data_and_cat[0])\n",
    "            if var_plus_genes:\n",
    "                var_plus_genes  = list(set(var_plus_genes))\n",
    "                var_plus_genes = \"'\" + \"', '\".join(var_plus_genes) + \"'\"\n",
    "            else:\n",
    "                var_plus_genes = ''\n",
    "                \n",
    "            for data_and_cat in custom_mut_extract.get_genes(sentence.strip()):\n",
    "                all_genes.append(data_and_cat[0])\n",
    "            if all_genes:\n",
    "                all_genes  = list(set(all_genes))\n",
    "                all_genes = \"'\" + \"', '\".join(all_genes) + \"'\"\n",
    "            else:\n",
    "                all_genes = ''   \n",
    "                \n",
    "        output = regex_block(sentence.strip())\n",
    "        if output:\n",
    "            mutations = []\n",
    "            for mut_and_snip in output:\n",
    "                # temp fix to deal with same mutation getting detected due to stiching multiple sentences\n",
    "                if (mut_and_snip[0] not in final[-1][4][1:-1].split(\", \") and mut_and_snip[0] not in final[-2][4][1:-1].split(\", \")) \\\n",
    "                            and mut_and_snip[0] not in mutations:\n",
    "                    mutations.append(mut_and_snip[0])\n",
    "            if mutations:\n",
    "                mutations = \"'\" + \"', '\".join(mutations) + \"'\"\n",
    "                if print_snips: print(1, mutations)\n",
    "                final.append([paper_id, 'Regex', all_genes, var_plus_genes, mutations, 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "            break\n",
    "\n",
    "        output = ner_score(sentence.strip())\n",
    "        if output:\n",
    "            mutations = []\n",
    "            for mut_and_snip in output:\n",
    "                # temp fix to deal with same mutation getting detected due to stiching multiple sentences\n",
    "                if (mut_and_snip[0] not in final[-1][4][1:-1].split(\", \") and mut_and_snip[0] not in final[-2][4][1:-1].split(\", \")) \\\n",
    "                        and not all(len(word) < 4 for word in mut_and_snip[0].split())\\\n",
    "                    and mut_and_snip[0] not in mutations:\n",
    "                    mutations.append(mut_and_snip[0])\n",
    "            if mutations:\n",
    "                mutations = \"'\" + \"', '\".join(mutations) + \"'\"\n",
    "                if print_snips: print(2, mutations)\n",
    "                final.append([paper_id, 'NER', all_genes, var_plus_genes, mutations, 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "            break\n",
    "        \n",
    "        # these data, if found, are going to be important if no mutations are in that sentence\n",
    "        if var_plus_genes or all_genes:\n",
    "            final.append([paper_id, '', all_genes, var_plus_genes, '', 'Line '+str(ppr_sen_idx)+': '+sentence.strip()])\n",
    "        \n",
    "        not_single_sentence = True\n",
    "print('Total time for processing in minutes: ', int((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final[2:] # removing the temporary first row and header\n",
    "\n",
    "# this sheet will contain high number of duplicates - which will get filtered in 3rd notebook\n",
    "# columns with asterisk contain data which are useful regardless of whether the sentence has  mutation info\n",
    "temp = pd.DataFrame(temp[:], columns=['WBPaper ID', 'Method', '* Genes', '* Gene-Variant combo', 'Mutation', 'Sentence'])\n",
    "temp.to_csv(\"data/model_output/extracted_snippets_part3.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90bac3f7a4bb879b9d06605bdeda624e0779c88b1a5b8631d7aaa6d430fa2aec"
  },
  "kernelspec": {
   "display_name": "wb_env",
   "language": "python",
   "name": "wb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
